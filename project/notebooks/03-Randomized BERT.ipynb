{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80642565-731f-433e-b037-69956a2c328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59818570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1.],\n",
       "        [0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(2, 4)\n",
    "a[:, range(1, a.size()[1], 2)] = 1.\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6f8121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180ce2f1-2f14-42c9-8c28-8f670c7ef694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-german-dbmdz-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_en = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model_de = BertModel.from_pretrained(\"bert-base-german-dbmdz-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03c5a4b-384c-4441-91eb-8ec9ac700e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6dc0aef-0bcf-4357-996c-7af4a46b3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def row_shuffling(tensor):\n",
    "    return tensor[torch.randperm(tensor.size()[0])]\n",
    "\n",
    "def col_shuffling(tensor):\n",
    "#     print(tensor.size(), torch.randperm(tensor.size()[1]))\n",
    "    if len(tensor.size()) == 2:\n",
    "        return tensor[:, torch.randperm(tensor.size()[1])]\n",
    "    elif len(tensor.size()) == 1:\n",
    "        return tensor[torch.randperm(tensor.size()[0])]\n",
    "    else:\n",
    "        raise ValueError(f\"tensor contains {len(tensor.size())} dimensions\")\n",
    "\n",
    "        \n",
    "def col_zeroing(tensor):\n",
    "#     if len(tensor.size()) == 2:\n",
    "#         zt[:, range(0, tensor.size()[1], 2)] = 1.\n",
    "#         res = tensor * zt\n",
    "#     elif len(tensor.size()) == 1:\n",
    "#         zt[range(0, tensor.size()[0], 2)] = 1.\n",
    "#         res = tensor * zt\n",
    "#     else:\n",
    "#         raise ValueError(f\"tensor contains {len(tensor.size())} dimensions\")\n",
    "#     return res\n",
    "    zt = torch.zeros(tensor.size())\n",
    "    if len(tensor.size()) == 2:\n",
    "        fsz = tensor.size(0)\n",
    "        ssz = tensor.size(1)\n",
    "        if fsz == 768 and ssz == 768:\n",
    "            zt[:, range(0, tensor.size(1), 2)] = 1.\n",
    "            zt[0] = 0.\n",
    "            zt[range(2, tensor.size(0), 2), :] = 0.\n",
    "            res = tensor * zt\n",
    "        elif fsz == 768 and ssz != 768:\n",
    "            zt[0] = 0.\n",
    "            zt[range(2, tensor.size(0), 2), :] = 0.\n",
    "            res = tensor * zt\n",
    "        elif ssz == 768 and fsz != 768:\n",
    "            zt[:, range(0, tensor.size(1), 2)] = 1.\n",
    "            res = tensor * zt\n",
    "        else:\n",
    "            res = tensor\n",
    "    elif len(tensor.size()) == 1:\n",
    "        if tensor.size(0) == 768:\n",
    "            zt[range(0, tensor.size(0), 2)] = 1.\n",
    "            res = tensor * zt\n",
    "        else:\n",
    "            res = tensor\n",
    "    else:\n",
    "        raise ValueError(f\"tensor contains {len(tensor.size())} dimensions\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def reduce_col(tensor):\n",
    "    # print(tensor[:, range(1, tensor.size()[1])].data)\n",
    "    if len(tensor.size()) == 2:\n",
    "        fsz = tensor.size(0)\n",
    "        ssz = tensor.size(1)\n",
    "        if fsz == 768 and ssz == 768:\n",
    "            zt = torch.zeros((tensor.size(0), fsz // 2))\n",
    "            zt = tensor[:, range(0, tensor.size(1), 2)]\n",
    "            sz = zt.size(0)\n",
    "            ft = torch.zeros((fsz // 2, zt.size(1)))\n",
    "            ft = zt[range(0, zt.size(0), 2), :]\n",
    "        elif fsz == 768 and ssz != 768:\n",
    "            ft = torch.zeros((fsz // 2, tensor.size(1)))\n",
    "            ft = tensor[range(0, tensor.size(0), 2), :]\n",
    "        elif ssz == 768 and fsz != 768:\n",
    "            ft = torch.zeros((tensor.size(0), ssz // 2))\n",
    "            ft = tensor[:, range(0, tensor.size(1), 2)]\n",
    "        else:\n",
    "            ft = tensor\n",
    "    elif len(tensor.size()) == 1:\n",
    "        if tensor.size(0) == 768:\n",
    "            fsz = tensor.size(0)\n",
    "            ft = torch.zeros(fsz // 2)\n",
    "            ft = tensor[range(0, tensor.size(0), 2)]\n",
    "        else:\n",
    "            ft = tensor\n",
    "    else:\n",
    "        raise ValueError(f\"tensor contains {len(tensor.size())} dimensions\")\n",
    "    return ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb7e0934-9e75-45e7-9730-b3f5e16f8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_params_en = {}\n",
    "# for k, v in model_en.named_parameters():\n",
    "#     new_params_en[k] = reduce_col(v)\n",
    "# new_params_en[\"embeddings.position_ids\"] = model_en.embeddings.position_ids\n",
    "\n",
    "# new_params_de = {}\n",
    "# for k, v in model_de.named_parameters():\n",
    "#     new_params_de[k] = reduce_col(v)\n",
    "# new_params_de[\"embeddings.position_ids\"] = model_de.embeddings.position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd76fdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params_en = {}\n",
    "for k, v in model_en.named_parameters():\n",
    "#     new_params_en[k] = col_zeroing(v)\n",
    "    if \"encoder.layer\" in k:\n",
    "        new_params_en[k] = col_zeroing(v)\n",
    "    else:\n",
    "        new_params_en[k] = v\n",
    "new_params_en[\"embeddings.position_ids\"] = model_en.embeddings.position_ids\n",
    "\n",
    "new_params_de = {}\n",
    "for k, v in model_de.named_parameters():\n",
    "#     new_params_de[k] = col_zeroing(v)\n",
    "    if \"encoder.layer\" in k:\n",
    "        new_params_de[k] = col_zeroing(v)\n",
    "    else:\n",
    "        new_params_de[k] = v\n",
    "new_params_de[\"embeddings.position_ids\"] = model_de.embeddings.position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5074071-588d-4606-a8aa-5dc844084a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n",
      "embeddings.position_ids torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "for k, v in new_params_en.items():\n",
    "    print(k, v.size())\n",
    "# new_params_en[\"embeddings.word_embeddings.weight\"]\n",
    "# new_params_en[\"encoder.layer.0.attention.self.query.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c76228ce-7acf-4eb9-a592-a0fc946c3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_params_en = {}\n",
    "# for k, v in model_en.named_parameters():\n",
    "#     if \"encoder.layer\" in k:\n",
    "#         new_params_en[k] = col_shuffling(v)\n",
    "#     else:\n",
    "#         new_params_en[k] = v\n",
    "        \n",
    "# new_params_de = {}\n",
    "# for k, v in model_de.named_parameters():\n",
    "#     if \"encoder.layer\" in k:\n",
    "#         new_params_de[k] = row_shuffling(col_shuffling(v))\n",
    "#     else:\n",
    "#         new_params_de[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252991e1-388e-4733-9cd4-b3a337885219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47534208, 47756928)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_en = BertModel(BertConfig(hidden_size=384))\n",
    "bert_en.load_state_dict(new_params_en)\n",
    "# bert_en.save_pretrained(\"./bert_en_zero_reduced\")\n",
    "\n",
    "bert_de = BertModel(BertConfig(vocab_size=31102, hidden_size=384))\n",
    "bert_de.load_state_dict(new_params_de)\n",
    "# bert_de.save_pretrained(\"./bert_de_zero_reduced\")\n",
    "count_parameters(bert_en), count_parameters(bert_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da5f99cf-3544-4fe6-a743-2371be8639ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en.load_state_dict(new_params_en, strict=False)\n",
    "model_de.load_state_dict(new_params_de, strict=False)\n",
    "\n",
    "count_parameters(model_en), count_parameters(model_de)\n",
    "\n",
    "model_de.save_pretrained(\"./bert_de_zeroed\")\n",
    "model_en.save_pretrained(\"./bert_en_zeroed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a98fe707-3e27-44e5-a6e2-ac391fe32d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"adapters\": {\n",
       "    \"adapters\": {},\n",
       "    \"config_map\": {},\n",
       "    \"fusion_config_map\": {},\n",
       "    \"fusions\": {}\n",
       "  },\n",
       "  \"architectures\": [\n",
       "    \"BertModel\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.11.3\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_en.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e8f5316-dde5-428b-bac4-b7acc001fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"./bert_de_zeroed/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09b09653-3b6a-4391-9f8c-6df536e07d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for k, v in x.items():\n",
    "#     print(k)\n",
    "(x[\"encoder.layer.11.intermediate.dense.bias\"] == model_de.encoder.layer[11].intermediate.dense.bias).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "504a1ac3-b608-4a46-9f37-c051da0e5429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0948, -0.0000, -0.0867,  ..., -0.0000, -0.0841, -0.0000])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"encoder.layer.11.intermediate.dense.bias\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b2bc76b2-a62f-48b9-98b8-02718c2d1515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0948, -0.0831, -0.0867,  ..., -0.0764, -0.0841, -0.0663],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_de.encoder.layer[11].intermediate.dense.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a955be84-693e-41b4-ba4c-4da86c08b031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0065,  0.0069, -0.0739,  ..., -0.0475, -0.0874, -0.0548],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_de.load_state_dict(x)\n",
    "model_de.encoder.layer[11].intermediate.dense.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab400854-9f77-44c3-8f16-c5afccc0b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "ln = nn.LayerNorm(768)\n",
    "ln_weights = ln.weight\n",
    "ln_bias = ln.bias\n",
    "inp = torch.randn(768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e041e5f9-c4d9-41ca-bd59-99561edc9e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8763,  0.0000,  1.3390,  0.0000, -0.7871,  0.0000,  1.0209,  0.0000,\n",
       "         0.5552,  0.0000, -1.4176,  0.0000,  0.6796,  0.0000,  0.6423,  0.0000,\n",
       "         1.1078,  0.0000, -0.5179,  0.0000,  1.3573,  0.0000,  1.4751,  0.0000,\n",
       "        -1.0862,  0.0000, -0.2637,  0.0000,  1.1915,  0.0000, -3.3621,  0.0000,\n",
       "         0.7017,  0.0000,  0.4402,  0.0000,  0.9803,  0.0000,  0.2810,  0.0000,\n",
       "        -0.6417,  0.0000, -0.6505,  0.0000,  1.3736,  0.0000,  0.0330,  0.0000,\n",
       "        -2.1577,  0.0000,  0.4948,  0.0000, -0.3180,  0.0000,  0.7448,  0.0000,\n",
       "        -0.2508,  0.0000, -0.4413,  0.0000,  1.1103,  0.0000, -0.6510,  0.0000,\n",
       "        -2.6974,  0.0000,  1.6307,  0.0000, -0.1122,  0.0000,  1.5548,  0.0000,\n",
       "         0.2812,  0.0000, -0.2508,  0.0000, -1.0473,  0.0000,  0.6684,  0.0000,\n",
       "         1.9529,  0.0000, -0.5952,  0.0000,  1.8745,  0.0000,  0.0681,  0.0000,\n",
       "        -0.9652,  0.0000,  0.5980,  0.0000, -0.7146,  0.0000,  0.4227,  0.0000,\n",
       "        -1.0849,  0.0000, -1.6329,  0.0000,  0.5987,  0.0000,  0.4564,  0.0000,\n",
       "         0.7279,  0.0000,  0.5230,  0.0000,  0.2282,  0.0000,  0.8415,  0.0000,\n",
       "         0.1358,  0.0000, -1.9529,  0.0000,  0.9109,  0.0000,  0.1174,  0.0000,\n",
       "         0.3903,  0.0000,  0.6940,  0.0000, -0.9127,  0.0000, -1.4909,  0.0000,\n",
       "         1.3918,  0.0000, -1.6475,  0.0000,  0.2103,  0.0000,  1.0469,  0.0000,\n",
       "         0.8797,  0.0000,  0.3110,  0.0000,  0.5212,  0.0000,  0.0204,  0.0000,\n",
       "        -1.3989,  0.0000,  0.6423,  0.0000,  0.5813,  0.0000,  1.4146,  0.0000,\n",
       "         0.7254,  0.0000,  0.8124,  0.0000,  1.1879,  0.0000,  1.4535,  0.0000,\n",
       "        -0.0118,  0.0000, -0.3089,  0.0000,  0.7412,  0.0000,  0.1867,  0.0000,\n",
       "        -0.7747,  0.0000, -0.7028,  0.0000, -0.3920,  0.0000, -1.4009,  0.0000,\n",
       "        -0.2320,  0.0000, -0.0284,  0.0000, -0.9827,  0.0000, -0.9169,  0.0000,\n",
       "         0.0766,  0.0000,  2.3721,  0.0000, -0.4224,  0.0000, -1.1515,  0.0000,\n",
       "        -0.9660,  0.0000,  0.9108,  0.0000,  1.6723,  0.0000, -0.2554,  0.0000,\n",
       "        -1.6418,  0.0000, -0.8199,  0.0000,  0.4927,  0.0000, -0.0050,  0.0000,\n",
       "        -0.7978,  0.0000, -1.0800,  0.0000, -0.2146,  0.0000,  1.4969,  0.0000,\n",
       "        -1.9050,  0.0000, -1.9535,  0.0000, -0.6996,  0.0000,  0.4093,  0.0000,\n",
       "        -1.5412,  0.0000,  0.4284,  0.0000,  1.4524,  0.0000, -0.8556,  0.0000,\n",
       "         0.1442,  0.0000, -0.4310,  0.0000,  1.1478,  0.0000, -0.3170,  0.0000,\n",
       "        -0.2935,  0.0000,  0.3989,  0.0000, -0.9351,  0.0000, -0.5002,  0.0000,\n",
       "         1.2153,  0.0000,  0.0378,  0.0000,  1.8614,  0.0000,  1.7361,  0.0000,\n",
       "        -0.1027,  0.0000, -0.2091,  0.0000,  0.0182,  0.0000,  0.4650,  0.0000,\n",
       "        -1.5363,  0.0000,  0.4906,  0.0000,  1.6669,  0.0000,  0.4727,  0.0000,\n",
       "         0.9379,  0.0000,  0.2721,  0.0000,  0.6573,  0.0000, -0.2447,  0.0000,\n",
       "        -0.1328,  0.0000,  1.4966,  0.0000, -1.0523,  0.0000, -1.2543,  0.0000,\n",
       "        -0.8556,  0.0000, -0.2616,  0.0000, -0.9700,  0.0000, -1.3903,  0.0000,\n",
       "         0.7007,  0.0000, -0.8413,  0.0000,  0.8863,  0.0000,  0.0233,  0.0000,\n",
       "        -1.7055,  0.0000, -0.5154,  0.0000, -0.4290,  0.0000, -2.1736,  0.0000,\n",
       "         2.3537,  0.0000,  1.1694,  0.0000,  0.4912,  0.0000,  1.0313,  0.0000,\n",
       "         0.9819,  0.0000,  0.2641,  0.0000,  0.3216,  0.0000,  0.0664,  0.0000,\n",
       "        -1.5048,  0.0000,  0.1774,  0.0000, -1.0483,  0.0000, -0.5807,  0.0000,\n",
       "        -1.1996,  0.0000, -1.4540,  0.0000,  0.8723,  0.0000, -1.4425,  0.0000,\n",
       "        -1.1686,  0.0000,  1.6549,  0.0000, -0.2913,  0.0000, -0.1854,  0.0000,\n",
       "        -1.1894,  0.0000, -1.1107,  0.0000,  0.3367,  0.0000,  0.8122,  0.0000,\n",
       "        -0.4464,  0.0000, -0.5320,  0.0000,  2.3934,  0.0000,  0.1217,  0.0000,\n",
       "         0.0761,  0.0000, -0.5859,  0.0000,  0.4784,  0.0000, -0.2359,  0.0000,\n",
       "         0.1116,  0.0000, -0.8414,  0.0000,  3.0984,  0.0000, -0.5875,  0.0000,\n",
       "        -0.5858,  0.0000,  0.7185,  0.0000,  1.4102,  0.0000, -0.0662,  0.0000,\n",
       "        -1.1621,  0.0000,  0.2900,  0.0000,  0.1809,  0.0000,  1.3601,  0.0000,\n",
       "        -0.7077,  0.0000,  0.4244,  0.0000,  1.3774,  0.0000, -0.1161,  0.0000,\n",
       "        -1.8720,  0.0000, -0.1975,  0.0000,  0.4862,  0.0000,  0.8922,  0.0000,\n",
       "         0.2607,  0.0000, -0.5716,  0.0000,  0.9708,  0.0000,  0.1738,  0.0000,\n",
       "        -1.7227,  0.0000, -0.1087,  0.0000, -0.5456,  0.0000, -2.1674,  0.0000,\n",
       "         1.4942,  0.0000,  0.9388,  0.0000,  0.1211,  0.0000,  0.7818,  0.0000,\n",
       "         1.4061,  0.0000, -0.1260,  0.0000,  2.6560,  0.0000, -0.0955,  0.0000,\n",
       "         1.1954,  0.0000, -0.3769,  0.0000, -1.0077,  0.0000,  0.1771,  0.0000,\n",
       "        -1.8401,  0.0000, -0.8204,  0.0000, -0.9926,  0.0000, -0.4404,  0.0000,\n",
       "        -0.1968,  0.0000,  0.5840,  0.0000, -1.2921,  0.0000, -0.7158,  0.0000,\n",
       "        -0.5523,  0.0000,  0.7803,  0.0000, -0.4517,  0.0000, -1.3235,  0.0000,\n",
       "        -1.7488,  0.0000,  0.1949,  0.0000,  1.3219,  0.0000,  0.3842,  0.0000,\n",
       "         0.0290,  0.0000,  1.1834,  0.0000, -0.1144,  0.0000,  0.5183,  0.0000,\n",
       "         0.0131,  0.0000, -0.9927,  0.0000,  0.5946,  0.0000,  0.7151,  0.0000,\n",
       "        -0.6050,  0.0000, -1.0988,  0.0000,  0.6531,  0.0000, -0.2112,  0.0000,\n",
       "         1.3722,  0.0000,  1.3999,  0.0000,  1.6211,  0.0000,  0.1423,  0.0000,\n",
       "        -0.9472,  0.0000, -2.0066,  0.0000, -0.9559,  0.0000, -0.6484,  0.0000,\n",
       "         0.2906,  0.0000, -0.4184,  0.0000,  1.2504,  0.0000,  1.9568,  0.0000,\n",
       "        -1.4511,  0.0000, -0.3124,  0.0000, -1.2969,  0.0000, -1.7777,  0.0000,\n",
       "        -0.4351,  0.0000, -1.4256,  0.0000, -0.1374,  0.0000, -0.2500,  0.0000,\n",
       "        -1.5865,  0.0000, -0.0052,  0.0000, -0.2801,  0.0000, -0.4890,  0.0000,\n",
       "         0.0941,  0.0000,  0.3269,  0.0000,  0.4793,  0.0000,  1.3085,  0.0000,\n",
       "         0.2306,  0.0000, -0.1075,  0.0000, -0.2726,  0.0000,  0.5354,  0.0000,\n",
       "         0.4924,  0.0000, -1.2493,  0.0000,  1.3273,  0.0000, -1.0742,  0.0000,\n",
       "        -0.0859,  0.0000, -2.2987,  0.0000, -1.5419,  0.0000, -0.5734,  0.0000,\n",
       "        -1.5996,  0.0000, -1.0573,  0.0000,  0.1945,  0.0000, -0.2566,  0.0000,\n",
       "         0.8742,  0.0000, -1.8876,  0.0000,  0.4844,  0.0000,  0.0796,  0.0000,\n",
       "        -1.7723,  0.0000,  0.0591,  0.0000, -0.5446,  0.0000,  1.1289,  0.0000,\n",
       "         0.9723,  0.0000,  1.0414,  0.0000, -0.4959,  0.0000,  0.8409,  0.0000,\n",
       "         1.5982,  0.0000,  0.7925,  0.0000,  0.7833,  0.0000, -0.4581,  0.0000,\n",
       "         0.0321,  0.0000, -1.1646,  0.0000,  1.3086,  0.0000, -0.6528,  0.0000,\n",
       "         0.1227,  0.0000,  0.6147,  0.0000,  0.4274,  0.0000,  0.1747,  0.0000,\n",
       "        -0.6223,  0.0000, -3.1410,  0.0000, -0.3777,  0.0000,  0.0927,  0.0000,\n",
       "        -1.3637,  0.0000, -0.6879,  0.0000, -0.7889,  0.0000,  0.2071,  0.0000,\n",
       "        -0.8514,  0.0000, -1.0808,  0.0000, -0.1551,  0.0000, -1.7127,  0.0000,\n",
       "         0.2191,  0.0000,  1.1568,  0.0000, -0.3593,  0.0000, -0.5792,  0.0000,\n",
       "         0.1305,  0.0000,  0.0162,  0.0000, -0.6610,  0.0000,  2.0845,  0.0000,\n",
       "        -0.4091,  0.0000,  0.1586,  0.0000,  1.6667,  0.0000, -0.6513,  0.0000,\n",
       "         0.1006,  0.0000,  0.3338,  0.0000, -1.3324,  0.0000, -0.6503,  0.0000,\n",
       "        -1.9688,  0.0000, -0.0142,  0.0000,  0.7165,  0.0000, -0.6936,  0.0000,\n",
       "         1.2534,  0.0000,  0.2676,  0.0000,  1.4415,  0.0000,  0.9529,  0.0000,\n",
       "        -0.0361,  0.0000, -0.3634,  0.0000, -1.2408,  0.0000, -0.7778,  0.0000,\n",
       "         1.0913,  0.0000,  0.6620,  0.0000, -0.1435,  0.0000,  0.5320,  0.0000,\n",
       "         2.2026,  0.0000,  0.1815,  0.0000,  0.9199,  0.0000,  0.7056,  0.0000,\n",
       "         0.6666,  0.0000,  1.4380,  0.0000, -0.5500,  0.0000, -0.4964,  0.0000,\n",
       "         0.8325,  0.0000,  1.2039,  0.0000,  0.6523,  0.0000, -1.4373,  0.0000,\n",
       "         0.3636,  0.0000,  0.9583,  0.0000,  0.2859,  0.0000,  1.0264,  0.0000],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln.weight.data = nn.Parameter(col_zeroing(ln_weights))\n",
    "ln.bias = nn.Parameter(col_zeroing(ln_bias))\n",
    "ln(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeeaa19a-b3cc-4d50-8eea-813606d44346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6858e+01,  2.5707e+01, -1.4950e+01,  1.9623e+01,  1.0718e+01,\n",
       "        -2.7007e+01,  1.3097e+01,  1.2384e+01,  2.1285e+01, -9.8033e+00,\n",
       "         2.6056e+01,  2.8309e+01, -2.0671e+01, -4.9410e+00,  2.2885e+01,\n",
       "        -6.4191e+01,  1.3519e+01,  8.5197e+00,  1.8848e+01,  5.4735e+00,\n",
       "        -1.2171e+01, -1.2338e+01,  2.6368e+01,  7.3150e-01, -4.1159e+01,\n",
       "         9.5625e+00, -5.9810e+00,  1.4344e+01, -4.6958e+00, -8.3380e+00,\n",
       "         2.1332e+01, -1.2347e+01, -5.1481e+01,  3.1285e+01, -2.0437e+00,\n",
       "         2.9833e+01,  5.4775e+00, -4.6957e+00, -1.9926e+01,  1.2882e+01,\n",
       "         3.7446e+01, -1.1281e+01,  3.5946e+01,  1.4023e+00, -1.8355e+01,\n",
       "         1.1535e+01, -1.3564e+01,  8.1848e+00, -2.0646e+01, -3.1124e+01,\n",
       "         1.1550e+01,  8.8281e+00,  1.4021e+01,  1.0103e+01,  4.4643e+00,\n",
       "         1.6192e+01,  2.6975e+00, -3.7244e+01,  1.7519e+01,  2.3459e+00,\n",
       "         7.5654e+00,  1.3372e+01, -1.7351e+01, -2.8408e+01,  2.6717e+01,\n",
       "        -3.1404e+01,  4.1228e+00,  2.0120e+01,  1.6924e+01,  6.0490e+00,\n",
       "         1.0069e+01,  4.9058e-01, -2.6650e+01,  1.2384e+01,  1.1216e+01,\n",
       "         2.7153e+01,  1.3972e+01,  1.5635e+01,  2.2817e+01,  2.7895e+01,\n",
       "        -1.2523e-01, -5.8066e+00,  1.4275e+01,  3.6713e+00, -1.4713e+01,\n",
       "        -1.3338e+01, -7.3955e+00, -2.6688e+01, -4.3359e+00, -4.4162e-01,\n",
       "        -1.8692e+01, -1.7432e+01,  1.5652e+00,  4.5461e+01, -7.9757e+00,\n",
       "        -2.1919e+01, -1.8371e+01,  1.7518e+01,  3.2080e+01, -4.7820e+00,\n",
       "        -3.1295e+01, -1.5578e+01,  9.5219e+00,  4.4695e-03, -1.5155e+01,\n",
       "        -2.0551e+01, -4.0027e+00,  2.8725e+01, -3.6327e+01, -3.7256e+01,\n",
       "        -1.3278e+01,  7.9270e+00, -2.9371e+01,  8.2937e+00,  2.7875e+01,\n",
       "        -1.6261e+01,  2.8575e+00, -8.1403e+00,  2.2050e+01, -5.9610e+00,\n",
       "        -5.5115e+00,  7.7293e+00, -1.7781e+01, -9.4637e+00,  2.3340e+01,\n",
       "         8.2436e-01,  3.5695e+01,  3.3299e+01, -1.8624e+00, -3.8977e+00,\n",
       "         4.4935e-01,  8.9939e+00, -2.9276e+01,  9.4820e+00,  3.1976e+01,\n",
       "         9.1406e+00,  1.8036e+01,  5.3048e+00,  1.2670e+01, -4.5777e+00,\n",
       "        -2.4380e+00,  2.8720e+01, -2.0023e+01, -2.3884e+01, -1.6261e+01,\n",
       "        -4.9012e+00, -1.8447e+01, -2.6486e+01,  1.3499e+01, -1.5987e+01,\n",
       "         1.7048e+01,  5.4673e-01, -3.2512e+01, -9.7540e+00, -8.1034e+00,\n",
       "        -4.1465e+01,  4.5109e+01,  2.2463e+01,  9.4939e+00,  1.9821e+01,\n",
       "         1.8878e+01,  5.1515e+00,  6.2500e+00,  1.3705e+00, -2.8675e+01,\n",
       "         3.4927e+00, -1.9945e+01, -1.1003e+01, -2.2838e+01, -2.7704e+01,\n",
       "         1.6781e+01, -2.7483e+01, -2.2246e+01,  3.1746e+01, -5.4700e+00,\n",
       "        -3.4440e+00, -2.2643e+01, -2.1138e+01,  6.5401e+00,  1.5632e+01,\n",
       "        -8.4353e+00, -1.0073e+01,  4.5870e+01,  2.4274e+00,  1.5560e+00,\n",
       "        -1.1103e+01,  9.2490e+00, -4.4096e+00,  2.2342e+00, -1.5989e+01,\n",
       "         5.9350e+01, -1.1133e+01, -1.1101e+01,  1.3841e+01,  2.7068e+01,\n",
       "        -1.1651e+00, -2.2121e+01,  5.6457e+00,  3.5601e+00,  2.6109e+01,\n",
       "        -1.3433e+01,  8.2168e+00,  2.6441e+01, -2.1191e+00, -3.5696e+01,\n",
       "        -3.6766e+00,  9.3976e+00,  1.7162e+01,  5.0859e+00, -1.0829e+01,\n",
       "         1.8665e+01,  3.4248e+00, -3.2841e+01, -1.9783e+00, -1.0331e+01,\n",
       "        -4.1345e+01,  2.8673e+01,  1.8053e+01,  2.4167e+00,  1.5050e+01,\n",
       "         2.6990e+01, -2.3088e+00,  5.0891e+01, -1.7260e+00,  2.2961e+01,\n",
       "        -7.1055e+00, -1.9169e+01,  3.4880e+00, -3.5087e+01, -1.5587e+01,\n",
       "        -1.8879e+01, -8.3205e+00, -3.6626e+00,  1.1269e+01, -2.4608e+01,\n",
       "        -1.3586e+01, -1.0461e+01,  1.5022e+01, -8.5360e+00, -2.5207e+01,\n",
       "        -3.3340e+01,  3.8284e+00,  2.5379e+01,  7.4471e+00,  6.5531e-01,\n",
       "         2.2731e+01, -2.0864e+00,  1.0012e+01,  3.5186e-01, -1.8882e+01,\n",
       "         1.1472e+01,  1.3775e+01, -1.1468e+01, -2.0910e+01,  1.2589e+01,\n",
       "        -3.9379e+00,  2.6342e+01,  2.6871e+01,  3.1101e+01,  2.8228e+00,\n",
       "        -1.8013e+01, -3.8270e+01, -1.8179e+01, -1.2299e+01,  5.6585e+00,\n",
       "        -7.8996e+00,  2.4013e+01,  3.7521e+01, -2.7649e+01, -5.8723e+00,\n",
       "        -2.4698e+01, -3.3893e+01, -8.2198e+00, -2.7160e+01, -2.5263e+00,\n",
       "        -4.6799e+00, -3.0237e+01,  1.1214e-03, -5.2554e+00, -9.2499e+00,\n",
       "         1.9004e+00,  6.3521e+00,  9.2666e+00,  2.5123e+01,  4.5106e+00,\n",
       "        -1.9549e+00, -5.1117e+00,  1.0339e+01,  9.5162e+00, -2.3788e+01,\n",
       "         2.5483e+01, -2.0440e+01, -1.5412e+00, -4.3855e+01, -2.9384e+01,\n",
       "        -1.0864e+01, -3.0487e+01, -2.0117e+01,  3.8205e+00, -4.8065e+00,\n",
       "         1.6817e+01, -3.5994e+01,  9.3631e+00,  1.6225e+00, -3.3791e+01,\n",
       "         1.2304e+00, -1.0314e+01,  2.1688e+01,  1.8695e+01,  2.0016e+01,\n",
       "        -9.3826e+00,  1.6181e+01,  3.0663e+01,  1.5256e+01,  1.5080e+01,\n",
       "        -8.6585e+00,  7.1566e-01, -2.2169e+01,  2.5126e+01, -1.2382e+01,\n",
       "         2.4471e+00,  1.1855e+01,  8.2733e+00,  3.4426e+00, -1.1798e+01,\n",
       "        -5.9964e+01, -7.1225e+00,  1.8731e+00, -2.5976e+01, -1.3054e+01,\n",
       "        -1.4985e+01,  4.0608e+00, -1.6179e+01, -2.0566e+01, -2.8644e+00,\n",
       "        -3.2650e+01,  4.2916e+00,  2.2222e+01, -6.7694e+00, -1.0976e+01,\n",
       "         2.5967e+00,  4.1039e-01, -1.2539e+01,  3.9963e+01, -7.7223e+00,\n",
       "         3.1334e+00,  3.1973e+01, -1.2354e+01,  2.0248e+00,  6.4842e+00,\n",
       "        -2.5378e+01, -1.2335e+01, -3.7548e+01, -1.7039e-01,  1.3802e+01,\n",
       "        -1.3162e+01,  2.4069e+01,  5.2175e+00,  2.7666e+01,  1.8324e+01,\n",
       "        -5.8933e-01, -6.8484e+00, -2.3626e+01, -1.4772e+01,  2.0969e+01,\n",
       "         1.2760e+01, -2.6426e+00,  1.0274e+01,  4.2220e+01,  3.5712e+00,\n",
       "         1.7691e+01,  1.3594e+01,  1.2848e+01,  2.7599e+01, -1.0417e+01,\n",
       "        -9.3911e+00,  1.6021e+01,  2.3123e+01,  1.2574e+01, -2.7385e+01,\n",
       "         7.0541e+00,  1.8427e+01,  5.5685e+00,  1.9728e+01],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln2 = nn.LayerNorm(384)\n",
    "ln2.weight = nn.Parameter(reduce_col(ln_weights))\n",
    "ln2.bias = nn.Parameter(reduce_col(ln_bias))\n",
    "ln2(reduce_col(inp)) * (2 * 384 / torch.sqrt(torch.tensor(4 * 384)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
