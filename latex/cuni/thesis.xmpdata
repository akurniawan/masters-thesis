% Metadata to be stored in PDF, see documentation of the pdfx package for more details.

\Author{Aditya Kurniawan}
\Title{Adapting Pretrained Models for Machine Translation}
\Keywords{machine translation\sep adapters\sep transformer\sep transfer learning}
\Subject{Pre-trained language models received extensive attention in recent years. However, it is still challenging to incorporate a pre-trained model such as BERT into Natural Language Generation tasks. In this work, we investigate a recent method called adapters as an alternative for fine-tuning. Adapters are a promising approach that allows to fine-tune only a very small fraction of a pretrained network. We show that with proper initialization, adapters can help achieve better performance than training models from scratch while training substantially fewer weights than the original model. We further show that even randomly set weights used as the base models for fine-tuning we can achieve a similar performance to one of the baseline models, bypassing the need to train hundreds of millions of weights in the pre-training phase. Furthermore, we study the effectiveness of adapters in the Transformer model for machine translation task. We put adapters either in the encoder or the decoder only and we also attempt to down-scale the pre-trained model size to decrease GPU memory demands. We found that incorporating adapters in the encoder alone matches the performance of the setup when we include the adapters on both the encoder and decoder.
In our down-scaling study, we found that using only a half of the original pre-trained weights can positively impact the performance when fine-tuned with adapters. Our experiments show that we can get almost the same performance as the original BERT model after fine-tuning the cross-attention layer.}
\Publisher{Charles University}
