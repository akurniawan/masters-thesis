%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

@book{Andel07,
  title     = {Základy matematické statistiky},
  publisher = {Matfyzpress},
  year      = {2007},
  author    = {Anděl, J.},
  address   = {Praha},
  series    = {Druhé opravené vydání},
  isbn      = {80-7378-001-1}
}

@book{Andel98,
  title     = {Statistické metody},
  publisher = {Matfyzpress},
  year      = {1998},
  author    = {Anděl, J.},
  address   = {Praha},
  series    = {Druhé přepracované vydání},
  isbn      = {80-85863-27-8}
}

@article{Cox72,
  author  = {Cox, D. R.},
  title   = {Regression models and life-tables (with {D}iscussion)},
  journal = {Journal of the Royal Statistical Society, Series B},
  year    = {1972},
  volume  = {34},
  pages   = {187--220},
  number  = {2}
}

@article{DempsterLairdRubin77,
  author  = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  title   = {Maximum likelihood from incomplete data via the {EM} algorithm},
  journal = {Journal of the Royal Statistical Society, Series B},
  year    = {1977},
  volume  = {39},
  pages   = {1--38},
  number  = {1}
}

@article{Genberget08,
  author  = {Genberg, B. L. and Kulich, M. and Kawichai, S. and Modiba, P. and
             Chingono, A. and Kilonzo, G. P. and Richter, L. and Pettifor, A.
             and Sweat, M. and Celentano, D. D.},
  title   = {{HIV} risk behaviors in Sub-{S}aharan {A}frica and {N}orthern {T}hailand:
             {B}aseline behavioral data from project {A}ccept},
  journal = {Journal of Acquired Immune Deficiency Syndrome},
  year    = {2008},
  volume  = {49},
  pages   = {309--319}
}

@article{KaplanMeier58,
  author  = {Kaplan, E. L. and Meier, P.},
  title   = {Nonparametric estimation from incomplete observations},
  journal = {Journal of the American Statistical Association},
  year    = {1958},
  volume  = {53},
  pages   = {457--481},
  number  = {282}
}

@book{LehmannCasella98,
  title     = {Theory of Point Estimation},
  publisher = {Springer-Verlag},
  year      = {1998},
  author    = {Lehmann, E. L. and Casella, G.},
  address   = {New York},
  series    = {{S}econd {E}dition},
  isbn      = {0-387-98502-6}
}

@article{Student08,
  author  = {Student},
  title   = {On the probable error of the mean},
  journal = {Biometrika},
  year    = {1908},
  volume  = {6},
  pages   = {1-25}
}

% Remove above
@inproceedings{wolf2020transformers,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages     = {38--45}
}

@article{guo2021adaptive,
  title   = {Adaptive Adapters: An Efficient Way to Incorporate BERT Into Neural Machine Translation},
  author  = {Junliang Guo and Zhirui Zhang and Linli Xu and Boxing Chen and Enhong Chen},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year    = {2021},
  volume  = {29},
  pages   = {1740-1751}
}

@article{zhu2020incorporating,
  title   = {Incorporating BERT into Neural Machine Translation},
  author  = {Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wen-gang Zhou and Houqiang Li and Tie-Yan Liu},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2002.06823}
}

@inproceedings{nallapati2016abstractive,
  title     = {Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond},
  author    = {Ramesh Nallapati and Bowen Zhou and C{\'i}cero Nogueira dos Santos and Çaglar G{\"u}lçehre and Bing Xiang},
  booktitle = {CoNLL},
  year      = {2016}
}

@article{bahdanau2015neural,
  title   = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author  = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal = {CoRR},
  year    = {2015},
  volume  = {abs/1409.0473}
}

@inproceedings{parisi2018continual,
  title  = {Continual Lifelong Learning with Neural Networks: A Review},
  author = {G. I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and S. Wermter},
  year   = {2018}
}

@data{zheng2021using,
  doi       = {},
  url       = {http://dx.doi.org/},
  author    = {Xianrui Zheng and Yulan Liu and Deniz Gunceler and Daniel Willett},
  publisher = {IEEE Signal Processing Society SigPort},
  title     = {Using Synthetic Audio to Improve the Recognition of Out-of-vocabulary Words in End-to-end ASR Systems},
  year      = {2021}
}

@inproceedings{dai2015semi,
  author     = {Dai, Andrew M and Le, Quoc V},
  booktitle  = {Advances in Neural Information Processing Systems},
  editor     = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  title      = {Semi-supervised Sequence Learning},
  url        = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},
  volume     = {28},
  year       = {2015},
  bdsk-url-1 = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf}
}

@inproceedings{howard2018universal,
  title     = {Universal Language Model Fine-tuning for Text Classification},
  author    = {Howard, Jeremy  and
               Ruder, Sebastian},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P18-1031},
  doi       = {10.18653/v1/P18-1031},
  pages     = {328--339}
}

@inproceedings{radford2018improving,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018}
}

@inproceedings{shavlik2010transfer,
  title     = {Transfer Learning},
  author    = {Lisa Torrey and Jude Shavlik},
  booktitle = {Handbook of research on
               machine learning applications and trends: algorithms, methods, and techniques},
  year      = {2010}
}

@inproceedings{gao2002improving,
  title     = {Improving Language Model Size Reduction using Better Pruning Criteria},
  author    = {Jianfeng Gao and Min Zhang},
  booktitle = {ACL},
  year      = {2002}
}

@article{Servan2016DomainSA,
  title   = {Domain specialization: a post-training domain adaptation for Neural Machine Translation},
  author  = {Christophe Servan and Josep Maria Crego and Jean Senellart},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1612.06141}
}

@inproceedings{Radford2018ImprovingLU,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018}
}

@inproceedings{Dai2015SemisupervisedSL,
  title     = {Semi-supervised Sequence Learning},
  author    = {Andrew M. Dai and Quoc V. Le},
  booktitle = {NIPS},
  year      = {2015}
}

@inproceedings{Howard2018UniversalLM,
  title     = {Universal Language Model Fine-tuning for Text Classification},
  author    = {Jeremy Howard and Sebastian Ruder},
  booktitle = {ACL},
  year      = {2018}
}

@inproceedings{Bowman2015ALA,
  title     = {A large annotated corpus for learning natural language inference},
  author    = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
  booktitle = {EMNLP},
  year      = {2015}
}

@inproceedings{Williams2018ABC,
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author    = {Adina Williams and Nikita Nangia and Samuel R. Bowman},
  booktitle = {NAACL},
  year      = {2018}
}

@inproceedings{Dolan2005AutomaticallyCA,
  title     = {Automatically Constructing a Corpus of Sentential Paraphrases},
  author    = {William B. Dolan and Chris Brockett},
  booktitle = {IJCNLP},
  year      = {2005}
}

@inproceedings{Chu2017AnEC,
  title     = {An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation},
  author    = {Chenhui Chu and Raj Dabre and Sadao Kurohashi},
  booktitle = {ACL},
  year      = {2017}
}

@article{Freitag2016FastDA,
  title   = {Fast Domain Adaptation for Neural Machine Translation},
  author  = {Markus Freitag and Yaser Al-Onaizan},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1612.06897}
}

@inproceedings{Chu2018ASO,
  title     = {A Survey of Domain Adaptation for Neural Machine Translation},
  author    = {Chenhui Chu and Rui Wang},
  booktitle = {COLING},
  year      = {2018}
}

@inproceedings{Sang2003IntroductionTT,
  title     = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Erik Tjong Kim Sang and Fien De Meulder},
  booktitle = {CoNLL},
  year      = {2003}
}

@inproceedings{Rajpurkar2016SQuAD1Q,
  title     = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author    = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle = {EMNLP},
  year      = {2016}
}

@inproceedings{Zhang2018LanguageMT,
  title     = {Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis},
  author    = {Kelly W. Zhang and Samuel R. Bowman},
  booktitle = {BlackboxNLP@EMNLP},
  year      = {2018}
}

@inproceedings{Wang2019CanYT,
  title     = {Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling},
  author    = {Alex Wang and Jan Hula and Patrick Xia and R. Pappagari and R. Thomas McCoy and Roma Patel and Najoung Kim and Ian Tenney and Yinghui Huang and Katherin Yu and Shuning Jin and Berlin Chen and Benjamin Van Durme and Edouard Grave and Ellie Pavlick and Samuel R. Bowman},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{Peters2018DeepCW,
  title     = {Deep Contextualized Word Representations},
  author    = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle = {NAACL},
  year      = {2018}
}

@article{Zhu2020IncorporatingBI,
  title   = {Incorporating BERT into Neural Machine Translation},
  author  = {Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wen-gang Zhou and Houqiang Li and Tie-Yan Liu},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2002.06823}
}

@article{Lee2020MixoutER,
  title   = {Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models},
  author  = {Cheolhyoung Lee and Kyunghyun Cho and Wanmo Kang},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/1909.11299}
}

@inproceedings{luong2015stanford,
  title     = {Stanford Neural Machine Translation Systems for Spoken Language Domains},
  author    = {Minh-Thang Luong and Christopher D. Manning},
  booktitle = {IWSLT},
  year      = {2015}
}

@inproceedings{hildebrand2005adaptation,
  title     = {Adaptation of the translation model for statistical machine translation based on information retrieval},
  author    = {Almut Silja Hildebrand and Matthias Eck and Stephan Vogel and Alexander H. Waibel},
  booktitle = {EAMT},
  year      = {2005}
}

@inproceedings{biesialska2020continual,
  title     = {Continual Lifelong Learning in Natural Language Processing: A Survey},
  author    = {Biesialska, Magdalena  and
               Biesialska, Katarzyna  and
               Costa-juss{\`a}, Marta R.},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = dec,
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.574},
  doi       = {10.18653/v1/2020.coling-main.574},
  pages     = {6523--6541}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{ratford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Alec Radford and Jeff Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year   = {2019}
}

@inproceedings{glorot2010understanding,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Xavier Glorot and Yoshua Bengio},
  booktitle = {AISTATS},
  year      = {2010}
}

@article{kumar2017onweight,
  author     = {Siddharth Krishna Kumar},
  title      = {On weight initialization in deep neural networks},
  journal    = {CoRR},
  volume     = {abs/1704.08863},
  year       = {2017},
  url        = {http://arxiv.org/abs/1704.08863},
  eprinttype = {arXiv},
  eprint     = {1704.08863},
  timestamp  = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Kumar17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{erhan2009thedifficulty,
  title     = {The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training},
  author    = {D. Erhan and Pierre-Antoine Manzagol and Yoshua Bengio and Samy Bengio and Pascal Vincent},
  booktitle = {AISTATS},
  year      = {2009}
}
@inproceedings{sutskever2014sequence,
  title     = {Sequence to Sequence Learning with Neural Networks},
  author    = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle = {NIPS},
  year      = {2014}
}
@inproceedings{cho2014properties,
  title     = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  author    = {Cho, Kyunghyun  and
               van Merri{\"e}nboer, Bart  and
               Bahdanau, Dzmitry  and
               Bengio, Yoshua},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W14-4012},
  doi       = {10.3115/v1/W14-4012},
  pages     = {103--111}
}

@inproceedings{bahdanau2015nmt,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin2018bert,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle = {NAACL},
  year      = {2019}
}

@article{mccloskey1989catastrophic,
  title   = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  author  = {M. McCloskey and N. Cohen},
  journal = {Psychology of Learning and Motivation},
  year    = {1989},
  volume  = {24},
  pages   = {109-165}
}

@article{yogatama2019learning,
  title   = {Learning and Evaluating General Linguistic Intelligence},
  author  = {Dani Yogatama and Cyprien de Masson d'Autume and Jerome T. Connor and Tom{\'a}s Kocisk{\'y} and Mike Chrzanowski and Lingpeng Kong and A. Lazaridou and Wang Ling and Lei Yu and Chris Dyer and P. Blunsom},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1901.11373}
}

@inproceedings{houlsby2019parameter,
  title        = {Parameter-efficient transfer learning for NLP},
  author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2790--2799},
  year         = {2019},
  organization = {PMLR}
}

@article{machavcek2018enriching,
  title     = {Enriching Neural MT through Multi-Task Training},
  author    = {Mach{\'a}{\v{c}}ek, Dominik},
  year      = {2018},
  publisher = {Univerzita Karlova, Matematicko-fyzik{\'a}ln{\'\i} fakulta}
}


@article{koehn2017nmt,
  author     = {Philipp Koehn},
  title      = {Neural Machine Translation},
  journal    = {CoRR},
  volume     = {abs/1709.07809},
  year       = {2017},
  url        = {http://arxiv.org/abs/1709.07809},
  eprinttype = {arXiv},
  eprint     = {1709.07809},
  timestamp  = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1709-07809.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{blunsom2013recurrent,
  title     = {Recurrent Continuous Translation Models},
  author    = {Kalchbrenner, Nal  and
               Blunsom, Phil},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  month     = oct,
  year      = {2013},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D13-1176},
  pages     = {1700--1709}
}

@inproceedings{bojar2015proceeding,
  title     = {Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal},
  author    = {Ondrej Bojar and Rajanya Chatterjee and Christian Federmann and Barry Haddow and Chris Hokamp and Matthias Huck and Varvara Logacheva and Pavel Pecina},
  booktitle = {WMT@EMNLP},
  year      = {2015}
}

@inproceedings{liu2020understanding,
  title     = {Understanding the Difficulty of Training Transformers},
  author    = {Liu, Liyuan  and
               Liu, Xiaodong  and
               Gao, Jianfeng  and
               Chen, Weizhu  and
               Han, Jiawei},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.463},
  doi       = {10.18653/v1/2020.emnlp-main.463},
  pages     = {5747--5763},
  abstract  = {Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand {\_}{\_}what complicates Transformer training{\_}{\_} from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially{---}for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage{'}s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance}
}

@inproceedings{ruder2019transfer,
  title     = {Transfer Learning in Natural Language Processing},
  author    = {Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages     = {15--18},
  year      = {2019}
}

@inproceedings{bapna2019simple,
  title     = {Simple, Scalable Adaptation for Neural Machine Translation},
  author    = {Bapna, Ankur  and
               Firat, Orhan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1165},
  doi       = {10.18653/v1/D19-1165},
  pages     = {1538--1548}
}

@inproceedings{han2021robust,
  title     = {Robust Transfer Learning with Pretrained Language Models through Adapters},
  author    = {Han, Wenjuan  and
               Pang, Bo  and
               Wu, Ying Nian},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-short.108},
  doi       = {10.18653/v1/2021.acl-short.108},
  pages     = {854--861},
  abstract  = {Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.}
}

@article{delange2021continual,
  author  = {Delange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Greg and Tuytelaars, Tinne},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {A continual learning survey: Defying forgetting in classification tasks},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TPAMI.2021.3057446}
}

@inproceedings{panayotov2015librispeech,
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Librispeech: An ASR corpus based on public domain audio books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {5206-5210},
  doi       = {10.1109/ICASSP.2015.7178964}
}

@inproceedings{pfeiffer2020madx,
  title     = {{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer},
  author    = {Pfeiffer, Jonas  and
               Vuli{\'c}, Ivan  and
               Gurevych, Iryna  and
               Ruder, Sebastian},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.617},
  doi       = {10.18653/v1/2020.emnlp-main.617},
  pages     = {7654--7673}
}

@article{ruckle2020adapterdrop,
  title   = {AdapterDrop: On the Efficiency of Adapters in Transformers},
  author  = {Andreas R{\"u}ckl{\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and N. Reimers and Iryna Gurevych},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2010.11918}
}

@article{canese2013pubmed,
  title     = {PubMed: the bibliographic database},
  author    = {Canese, Kathi and Weis, Sarah},
  journal   = {The NCBI Handbook},
  volume    = {2},
  pages     = {1},
  year      = {2013},
  publisher = {National Center for Biotechnology Information (US)}
}

@inproceedings{stolcke1998dialog,
  title     = {Dialog act modeling for conversational speech},
  author    = {Stolcke, Andreas and Shriberg, Elizabeth and Bates, Rebecca and Coccaro, Noah and Jurafsky, Daniel and Martin, Rachel and Meteer, Marie and Ries, Klaus and Taylor, Paul and Van Ess-Dykema, Carol and others},
  booktitle = {AAAI Spring Symposium on Applying Machine Learning to Discourse Processing},
  pages     = {98--105},
  year      = {1998}
}

@article{ii2020developingrm,
  title   = {Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability},
  author  = {Jinyu Li and Rui Zhao and Zhong Meng and Yanqing Liu and Wenning Wei and S. Parthasarathy and Vadim Mazalov and Z. Wang and Lei He and Sheng Zhao and Y. Gong},
  journal = {INTERSPEECH 2020},
  year    = {2020},
  volume  = {abs/2007.15188}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@inproceedings{pfeiffer2021adapterfusion,
  title     = {{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning},
  author    = {Pfeiffer, Jonas  and
               Kamath, Aishwarya  and
               R{\"u}ckl{\'e}, Andreas  and
               Cho, Kyunghyun  and
               Gurevych, Iryna},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month     = apr,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.eacl-main.39},
  pages     = {487--503}
}

@misc{winata2020adapt,
  title         = {Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition},
  author        = {Genta Indra Winata and Guangsen Wang and Caiming Xiong and Steven Hoi},
  year          = {2020},
  eprint        = {2012.01687},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{lee2021adaptable,
  title        = {Adaptable multi-domain language model for transformer asr},
  author       = {Lee, Taewoo and Lee, Min-Joong and Kang, Tae Gyoon and Jung, Seokyeoung and Kwon, Minseok and Hong, Yeona and Lee, Jungin and Woo, Kyoung-Gu and Kim, Ho-Gyeong and Jeong, Jiseung and others},
  booktitle    = {ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages        = {7358--7362},
  year         = {2021},
  organization = {IEEE}
}

@inproceedings{pham2020study,
  title     = {A study of residual adapters for multi-domain neural machine translation},
  author    = {Pham, Minh Quang and Crego, Josep M and Yvon, Fran{\c{c}}ois and Senellart, Jean},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation},
  pages     = {617--628},
  year      = {2020}
}

@article{french1999catastrophic,
  title     = {Catastrophic forgetting in connectionist networks},
  author    = {French, Robert M},
  journal   = {Trends in cognitive sciences},
  volume    = {3},
  number    = {4},
  pages     = {128--135},
  year      = {1999},
  publisher = {Elsevier}
}

@article{rosenfeld2018incremental,
  title     = {Incremental learning through deep adaptation},
  author    = {Rosenfeld, Amir and Tsotsos, John K},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {42},
  number    = {3},
  pages     = {651--663},
  year      = {2018},
  publisher = {IEEE}
}

@article{kirkpatrick2017overcoming,
  title     = {Overcoming catastrophic forgetting in neural networks},
  author    = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal   = {Proceedings of the national academy of sciences},
  volume    = {114},
  number    = {13},
  pages     = {3521--3526},
  year      = {2017},
  publisher = {National Acad Sciences}
}

@incollection{thrun1998lifelong,
  title     = {Lifelong learning algorithms},
  author    = {Thrun, Sebastian},
  booktitle = {Learning to learn},
  pages     = {181--209},
  year      = {1998},
  publisher = {Springer}
}

@article{rochan2021unsupervised,
  title   = {Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters},
  author  = {Rochan, Mrigank and Aich, Shubhra and Corral-Soto, Eduardo R and Nabatchian, Amir and Liu, Bingbing},
  journal = {arXiv preprint arXiv:2107.09783},
  year    = {2021}
}

@article{kudo2018sentencepiece,
  author     = {Taku Kudo and
                John Richardson},
  title      = {SentencePiece: {A} simple and language independent subword tokenizer
                and detokenizer for Neural Text Processing},
  journal    = {CoRR},
  volume     = {abs/1808.06226},
  year       = {2018},
  url        = {http://arxiv.org/abs/1808.06226},
  eprinttype = {arXiv},
  eprint     = {1808.06226},
  timestamp  = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1808-06226.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{agarap2018deep,
  title   = {Deep learning using rectified linear units (relu)},
  author  = {Agarap, Abien Fred},
  journal = {arXiv preprint arXiv:1803.08375},
  year    = {2018}
}

@article{lei2016layernorm,
  author     = {Lei Jimmy Ba and
                Jamie Ryan Kiros and
                Geoffrey E. Hinton},
  title      = {Layer Normalization},
  journal    = {CoRR},
  volume     = {abs/1607.06450},
  year       = {2016},
  url        = {http://arxiv.org/abs/1607.06450},
  eprinttype = {arXiv},
  eprint     = {1607.06450},
  timestamp  = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
