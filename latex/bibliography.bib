%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

% Remove above

@inproceedings{barrault-etal-2019-findings,
  title     = {Findings of the 2019 Conference on Machine Translation ({WMT}19)},
  author    = {Barrault, Lo{\"\i}c  and
               Bojar, Ond{\v{r}}ej  and
               Costa-juss{\`a}, Marta R.  and
               Federmann, Christian  and
               Fishel, Mark  and
               Graham, Yvette  and
               Haddow, Barry  and
               Huck, Matthias  and
               Koehn, Philipp  and
               Malmasi, Shervin  and
               Monz, Christof  and
               M{\"u}ller, Mathias  and
               Pal, Santanu  and
               Post, Matt  and
               Zampieri, Marcos},
  booktitle = {Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)},
  month     = aug,
  year      = {2019},
  address   = {Florence, Italy},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W19-5301},
  doi       = {10.18653/v1/W19-5301},
  pages     = {1--61},
  abstract  = {This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2019. Participants were asked to build machine translation systems for any of 18 language pairs, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. The task was also opened up to additional test suites to probe specific aspects of translation.}
}

@inproceedings{koehn-monz-2006-manual,
  title     = {Manual and Automatic Evaluation of Machine Translation between {E}uropean Languages},
  author    = {Koehn, Philipp  and
               Monz, Christof},
  booktitle = {Proceedings on the Workshop on Statistical Machine Translation},
  month     = jun,
  year      = {2006},
  address   = {New York City},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W06-3114},
  pages     = {102--121}
}

@inproceedings{callison-burch-etal-2007-meta,
  title     = {(Meta-) Evaluation of Machine Translation},
  author    = {Callison-Burch, Chris  and
               Fordyce, Cameron  and
               Koehn, Philipp  and
               Monz, Christof  and
               Schroeder, Josh},
  booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
  month     = jun,
  year      = {2007},
  address   = {Prague, Czech Republic},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W07-0718},
  pages     = {136--158}
}

@inproceedings{callison-burch-etal-2008-meta,
  title     = {Further Meta-Evaluation of Machine Translation},
  author    = {Callison-Burch, Chris  and
               Fordyce, Cameron  and
               Koehn, Philipp  and
               Monz, Christof  and
               Schroeder, Josh},
  booktitle = {Proceedings of the Third Workshop on Statistical Machine Translation},
  month     = jun,
  year      = {2008},
  address   = {Columbus, Ohio},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W08-0309},
  pages     = {70--106}
}

@inproceedings{callison-burch-etal-2009-findings,
  title     = {Findings of the 2009 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
  author    = {Callison-Burch, Chris  and
               Koehn, Philipp  and
               Monz, Christof  and
               Schroeder, Josh},
  booktitle = {Proceedings of the Fourth Workshop on Statistical Machine Translation},
  month     = mar,
  year      = {2009},
  address   = {Athens, Greece},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W09-0401},
  pages     = {1--28}
}

@inproceedings{callison-burch-etal-2010-findings,
  title     = {Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation},
  author    = {Callison-Burch, Chris  and
               Koehn, Philipp  and
               Monz, Christof  and
               Peterson, Kay  and
               Przybocki, Mark  and
               Zaidan, Omar},
  booktitle = {Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR}},
  month     = jul,
  year      = {2010},
  address   = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W10-1703},
  pages     = {17--53}
}

@inproceedings{callison-burch-etal-2011-findings,
  title     = {Findings of the 2011 Workshop on Statistical Machine Translation},
  author    = {Callison-Burch, Chris  and
               Koehn, Philipp  and
               Monz, Christof  and
               Zaidan, Omar},
  booktitle = {Proceedings of the Sixth Workshop on Statistical Machine Translation},
  month     = jul,
  year      = {2011},
  address   = {Edinburgh, Scotland},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W11-2103},
  pages     = {22--64}
}

@inproceedings{callison-burch-etal-2012-findings,
  title     = {Findings of the 2012 Workshop on Statistical Machine Translation},
  author    = {Callison-Burch, Chris  and
               Koehn, Philipp  and
               Monz, Christof  and
               Post, Matt  and
               Soricut, Radu  and
               Specia, Lucia},
  booktitle = {Proceedings of the Seventh Workshop on Statistical Machine Translation},
  month     = jun,
  year      = {2012},
  address   = {Montr{\'e}al, Canada},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W12-3102},
  pages     = {10--51}
}

@inproceedings{bojar-etal-2013-findings,
  title     = {Findings of the 2013 {W}orkshop on {S}tatistical {M}achine {T}ranslation},
  author    = {Bojar, Ond{\v{r}}ej  and
               Buck, Christian  and
               Callison-Burch, Chris  and
               Federmann, Christian  and
               Haddow, Barry  and
               Koehn, Philipp  and
               Monz, Christof  and
               Post, Matt  and
               Soricut, Radu  and
               Specia, Lucia},
  booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation},
  month     = aug,
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W13-2201},
  pages     = {1--44}
}

@inproceedings{bojar-etal-2014-findings,
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  author    = {Bojar, Ond{\v{r}}ej  and
               Buck, Christian  and
               Federmann, Christian  and
               Haddow, Barry  and
               Koehn, Philipp  and
               Leveling, Johannes  and
               Monz, Christof  and
               Pecina, Pavel  and
               Post, Matt  and
               Saint-Amand, Herve  and
               Soricut, Radu  and
               Specia, Lucia  and
               Tamchyna, Ale{\v{s}}},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = jun,
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W14-3302},
  doi       = {10.3115/v1/W14-3302},
  pages     = {12--58}
}

@inproceedings{bojar-etal-2015-findings,
  title     = {Findings of the 2015 Workshop on Statistical Machine Translation},
  author    = {Bojar, Ond{\v{r}}ej  and
               Chatterjee, Rajen  and
               Federmann, Christian  and
               Haddow, Barry  and
               Huck, Matthias  and
               Hokamp, Chris  and
               Koehn, Philipp  and
               Logacheva, Varvara  and
               Monz, Christof  and
               Negri, Matteo  and
               Post, Matt  and
               Scarton, Carolina  and
               Specia, Lucia  and
               Turchi, Marco},
  booktitle = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  month     = sep,
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W15-3001},
  doi       = {10.18653/v1/W15-3001},
  pages     = {1--46}
}

@inproceedings{bojar-etal-2016-findings,
  title     = {Findings of the 2016 Conference on Machine Translation},
  author    = {Bojar, Ond{\v{r}}ej  and
               Chatterjee, Rajen  and
               Federmann, Christian  and
               Graham, Yvette  and
               Haddow, Barry  and
               Huck, Matthias  and
               Jimeno Yepes, Antonio  and
               Koehn, Philipp  and
               Logacheva, Varvara  and
               Monz, Christof  and
               Negri, Matteo  and
               N{\'e}v{\'e}ol, Aur{\'e}lie  and
               Neves, Mariana  and
               Popel, Martin  and
               Post, Matt  and
               Rubino, Raphael  and
               Scarton, Carolina  and
               Specia, Lucia  and
               Turchi, Marco  and
               Verspoor, Karin  and
               Zampieri, Marcos},
  booktitle = {Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W16-2301},
  doi       = {10.18653/v1/W16-2301},
  pages     = {131--198}
}

@inproceedings{pfeiffer-etal-2022-lifting,
  title     = {Lifting the Curse of Multilinguality by Pre-training Modular Transformers},
  author    = {Pfeiffer, Jonas  and
               Goyal, Naman  and
               Lin, Xi  and
               Li, Xian  and
               Cross, James  and
               Riedel, Sebastian  and
               Artetxe, Mikel},
  booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = jul,
  year      = {2022},
  address   = {Seattle, United States},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2022.naacl-main.255},
  pages     = {3479--3495},
  abstract  = {Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-Mod) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.}
}


@inproceedings{ben-noach-goldberg-2020-compressing,
  title     = {Compressing Pre-trained Language Models by Matrix Decomposition},
  author    = {Ben Noach, Matan  and
               Goldberg, Yoav},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  month     = dec,
  year      = {2020},
  address   = {Suzhou, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.aacl-main.88},
  pages     = {884--889},
  abstract  = {Large pre-trained language models reach state-of-the-art results on many different NLP tasks when fine-tuned individually; They also come with a significant memory and computational requirements, calling for methods to reduce model sizes (green AI). We propose a two-stage model-compression method to reduce a model{'}s inference time cost. We first decompose the matrices in the model into smaller matrices and then perform feature distillation on the internal representation to recover from the decomposition. This approach has the benefit of reducing the number of parameters while preserving much of the information within the model. We experimented on BERT-base model with the GLUE benchmark dataset and show that we can reduce the number of parameters by a factor of 0.4x, and increase inference speed by a factor of 1.45x, while maintaining a minimal loss in metric performance.}
}

@inproceedings{ravfogel-etal-2020-unsupervised,
  title     = {Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},
  author    = {Ravfogel, Shauli  and
               Elazar, Yanai  and
               Goldberger, Jacob  and
               Goldberg, Yoav},
  booktitle = {Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.blackboxnlp-1.9},
  doi       = {10.18653/v1/2020.blackboxnlp-1.9},
  pages     = {91--106},
  abstract  = {Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.}
}



@article{Park2019SelfSupervisedCD,
  title   = {Self-Supervised Contextual Data Augmentation for Natural Language Processing},
  author  = {Dongju Park and Chang Wook Ahn},
  journal = {Symmetry},
  year    = {2019},
  volume  = {11},
  pages   = {1393}
}
@inproceedings{BLEU,
  author    = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  title     = {BLEU: A Method for Automatic Evaluation of Machine Translation},
  booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
  series    = {ACL '02},
  year      = {2002},
  location  = {Philadelphia, Pennsylvania},
  pages     = {311--318},
  numpages  = {8},
  url       = {https://doi.org/10.3115/1073083.1073135},
  doi       = {10.3115/1073083.1073135},
  acmid     = {1073135},
  publisher = {Association for Computational Linguistics},
  address   = {Stroudsburg, PA, USA}
} 

@inproceedings{post-2018-call,
  title     = {A Call for Clarity in Reporting {BLEU} Scores},
  author    = {Post, Matt},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
  month     = oct,
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W18-6319},
  pages     = {186--191}
}

@inproceedings{Kuchaiev2017FactorizationTF,
  author    = {Oleksii Kuchaiev and
               Boris Ginsburg},
  title     = {Factorization tricks for {LSTM} networks},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=ByxWXyNFg},
  timestamp = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/KuchaievG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Shazeer2017OutrageouslyLN,
  author    = {Noam Shazeer and
               Azalia Mirhoseini and
               Krzysztof Maziarz and
               Andy Davis and
               Quoc V. Le and
               Geoffrey E. Hinton and
               Jeff Dean},
  title     = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts
               Layer},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=B1ckMDqlg},
  timestamp = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ShazeerMMDLHD17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{bojar-etal-2017-findings,
  title     = {Findings of the 2017 Conference on Machine Translation ({WMT}17)},
  author    = {Bojar, Ond{\v{r}}ej  and
               Chatterjee, Rajen  and
               Federmann, Christian  and
               Graham, Yvette  and
               Haddow, Barry  and
               Huang, Shujian  and
               Huck, Matthias  and
               Koehn, Philipp  and
               Liu, Qun  and
               Logacheva, Varvara  and
               Monz, Christof  and
               Negri, Matteo  and
               Post, Matt  and
               Rubino, Raphael  and
               Specia, Lucia  and
               Turchi, Marco},
  booktitle = {Proceedings of the Second Conference on Machine Translation},
  month     = sep,
  year      = {2017},
  address   = {Copenhagen, Denmark},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W17-4717},
  doi       = {10.18653/v1/W17-4717},
  pages     = {169--214}
}

@inproceedings{bojar-etal-2018-findings,
  title     = {Findings of the 2018 Conference on Machine Translation ({WMT}18)},
  author    = {Bojar, Ond{\v{r}}ej  and
               Federmann, Christian  and
               Fishel, Mark  and
               Graham, Yvette  and
               Haddow, Barry  and
               Koehn, Philipp  and
               Monz, Christof},
  booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
  month     = oct,
  year      = {2018},
  address   = {Belgium, Brussels},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-6401},
  doi       = {10.18653/v1/W18-6401},
  pages     = {272--303},
  abstract  = {This paper presents the results of the premier shared task organized alongside the Conference on Machine Translation (WMT) 2018. Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation.}
}

@inproceedings{schuster2012japanese,
  title        = {Japanese and korean voice search},
  author       = {Schuster, Mike and Nakajima, Kaisuke},
  booktitle    = {2012 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages        = {5149--5152},
  year         = {2012},
  organization = {IEEE}
}

@inproceedings{pfeiffer-etal-2020-adapterhub,
  title     = {{A}dapter{H}ub: A Framework for Adapting Transformers},
  author    = {Pfeiffer, Jonas  and
               R{\"u}ckl{\'e}, Andreas  and
               Poth, Clifton  and
               Kamath, Aishwarya  and
               Vuli{\'c}, Ivan  and
               Ruder, Sebastian  and
               Cho, Kyunghyun  and
               Gurevych, Iryna},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-demos.7},
  doi       = {10.18653/v1/2020.emnlp-demos.7},
  pages     = {46--54},
  abstract  = {The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters{---}small learnt bottleneck layers inserted within each layer of a pre-trained model{---} ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic {``}stiching-in{''} of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml}
}


@inproceedings{Cettolo2014ReportOT,
  title     = {Report on the 11th IWSLT evaluation campaign},
  author    = {Mauro Cettolo and Jan Niehues and Sebastian St{\"u}ker and Luisa Bentivogli and Marcello Federico},
  booktitle = {IWSLT},
  year      = {2014}
}

@inproceedings{sennrich-etal-2016-neural,
  title     = {Neural Machine Translation of Rare Words with Subword Units},
  author    = {Sennrich, Rico  and
               Haddow, Barry  and
               Birch, Alexandra},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = aug,
  year      = {2016},
  address   = {Berlin, Germany},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P16-1162},
  doi       = {10.18653/v1/P16-1162},
  pages     = {1715--1725}
}


@inproceedings{wolf2020transformers,
  title     = {Transformers: State-of-the-Art Natural Language Processing},
  author    = {Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  month     = oct,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  pages     = {38--45}
}

@article{guo2021adaptive,
  title   = {Adaptive Adapters: An Efficient Way to Incorporate BERT Into Neural Machine Translation},
  author  = {Junliang Guo and Zhirui Zhang and Linli Xu and Boxing Chen and Enhong Chen},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year    = {2021},
  volume  = {29},
  pages   = {1740-1751}
}

@inproceedings{philip2020monolingual,
  title     = {Monolingual adapters for zero-shot neural machine translation},
  author    = {Philip, Jerin and Berard, Alexandre and Gall{\'e}, Matthias and Besacier, Laurent},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages     = {4465--4470},
  year      = {2020}
}

@inproceedings{conneau2019cross,
  author     = {CONNEAU, Alexis and Lample, Guillaume},
  booktitle  = {Advances in Neural Information Processing Systems},
  editor     = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  title      = {Cross-lingual Language Model Pretraining},
  url        = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
  volume     = {32},
  year       = {2019},
  bdsk-url-1 = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf}
}


@article{zhu2020incorporating,
  title   = {Incorporating BERT into Neural Machine Translation},
  author  = {Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wen-gang Zhou and Houqiang Li and Tie-Yan Liu},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2002.06823}
}

@inproceedings{nallapati2016abstractive,
  title     = {Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond},
  author    = {Ramesh Nallapati and Bowen Zhou and C{\'i}cero Nogueira dos Santos and Çaglar G{\"u}lçehre and Bing Xiang},
  booktitle = {CoNLL},
  year      = {2016}
}

@inproceedings{parisi2018continual,
  title  = {Continual Lifelong Learning with Neural Networks: A Review},
  author = {G. I. Parisi and Ronald Kemker and Jose L. Part and Christopher Kanan and S. Wermter},
  year   = {2018}
}

@data{zheng2021using,
  doi       = {},
  url       = {http://dx.doi.org/},
  author    = {Xianrui Zheng and Yulan Liu and Deniz Gunceler and Daniel Willett},
  publisher = {IEEE Signal Processing Society SigPort},
  title     = {Using Synthetic Audio to Improve the Recognition of Out-of-vocabulary Words in End-to-end ASR Systems},
  year      = {2021}
}

@inproceedings{dai2015semi,
  author     = {Dai, Andrew M and Le, Quoc V},
  booktitle  = {Advances in Neural Information Processing Systems},
  editor     = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  publisher  = {Curran Associates, Inc.},
  title      = {Semi-supervised Sequence Learning},
  url        = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf},
  volume     = {28},
  year       = {2015},
  bdsk-url-1 = {https://proceedings.neurips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf}
}

@inproceedings{howard2018universal,
  title     = {Universal Language Model Fine-tuning for Text Classification},
  author    = {Howard, Jeremy  and
               Ruder, Sebastian},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P18-1031},
  doi       = {10.18653/v1/P18-1031},
  pages     = {328--339}
}

@inproceedings{radford2018improving,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018}
}

@inproceedings{shavlik2010transfer,
  title     = {Transfer Learning},
  author    = {Lisa Torrey and Jude Shavlik},
  booktitle = {Handbook of research on
               machine learning applications and trends: algorithms, methods, and techniques},
  year      = {2010}
}

@inproceedings{gao2002improving,
  title     = {Improving Language Model Size Reduction using Better Pruning Criteria},
  author    = {Jianfeng Gao and Min Zhang},
  booktitle = {ACL},
  year      = {2002}
}

@article{Mosbach2021OnTS,
  title   = {On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines},
  author  = {Marius Mosbach and Maksym Andriushchenko and Dietrich Klakow},
  journal = {ArXiv},
  year    = {2021},
  volume  = {abs/2006.04884}
}

@article{chen2019distilling,
  title  = {Distilling the knowledge of bert for text generation},
  author = {Chen, Yen-Chun and Gan, Zhe and Cheng, Yu and Liu, Jingzhou and Liu, Jingjing},
  year   = {2019}
}


@inproceedings{yang2020towards,
  title     = {Towards making the most of bert in neural machine translation},
  author    = {Yang, Jiacheng and Wang, Mingxuan and Zhou, Hao and Zhao, Chengqi and Zhang, Weinan and Yu, Yong and Li, Lei},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {34},
  number    = {05},
  pages     = {9378--9385},
  year      = {2020}
}


@inproceedings{weng2020acquiring,
  title     = {Acquiring knowledge from pre-trained model to neural machine translation},
  author    = {Weng, Rongxiang and Yu, Heng and Huang, Shujian and Cheng, Shanbo and Luo, Weihua},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume    = {34},
  number    = {05},
  pages     = {9266--9273},
  year      = {2020}
}


@inproceedings{Barone2017RegularizationTF,
  title     = {Regularization techniques for fine-tuning in neural machine translation},
  author    = {Antonio Valerio Miceli Barone and Barry Haddow and Ulrich Germann and Rico Sennrich},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{rajpurkar2018know,
  title     = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
  author    = {Rajpurkar, Pranav  and
               Jia, Robin  and
               Liang, Percy},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  month     = jul,
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/P18-2124},
  doi       = {10.18653/v1/P18-2124},
  pages     = {784--789},
  abstract  = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.}
}


@inproceedings{wang2018glue,
  title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
  author    = {Wang, Alex  and
               Singh, Amanpreet  and
               Michael, Julian  and
               Hill, Felix  and
               Levy, Omer  and
               Bowman, Samuel},
  booktitle = {Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}},
  month     = nov,
  year      = {2018},
  address   = {Brussels, Belgium},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W18-5446},
  doi       = {10.18653/v1/W18-5446},
  pages     = {353--355},
  abstract  = {Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.}
}


@article{Sennrich2016ImprovingNM,
  title   = {Improving Neural Machine Translation Models with Monolingual Data},
  author  = {Rico Sennrich and Barry Haddow and Alexandra Birch},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1511.06709}
}

@article{Shazeer2018MeshTensorFlowDL,
  title   = {Mesh-TensorFlow: Deep Learning for Supercomputers},
  author  = {Noam M. Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake A. Hechtman},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1811.02084}
}

@inproceedings{Bapna2018TrainingDN,
  title     = {Training Deeper Neural Machine Translation Models with Transparent Attention},
  author    = {Ankur Bapna and Mia Xu Chen and Orhan Firat and Yuan Cao and Yonghui Wu},
  booktitle = {EMNLP},
  year      = {2018}
}

@article{Huang2019GPipeET,
  title   = {GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism},
  author  = {Yanping Huang and Yonglong Cheng and Dehao Chen and HyoukJoong Lee and Jiquan Ngiam and Quoc V. Le and Z. Chen},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1811.06965}
}

@article{Servan2016DomainSA,
  title   = {Domain specialization: a post-training domain adaptation for Neural Machine Translation},
  author  = {Christophe Servan and Josep Maria Crego and Jean Senellart},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1612.06141}
}

@inproceedings{Radford2018ImprovingLU,
  title  = {Improving Language Understanding by Generative Pre-Training},
  author = {Alec Radford and Karthik Narasimhan},
  year   = {2018}
}

@inproceedings{Dai2015SemisupervisedSL,
  title     = {Semi-supervised Sequence Learning},
  author    = {Andrew M. Dai and Quoc V. Le},
  booktitle = {NIPS},
  year      = {2015}
}

@inproceedings{Howard2018UniversalLM,
  title     = {Universal Language Model Fine-tuning for Text Classification},
  author    = {Jeremy Howard and Sebastian Ruder},
  booktitle = {ACL},
  year      = {2018}
}

@inproceedings{Bowman2015ALA,
  title     = {A large annotated corpus for learning natural language inference},
  author    = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning},
  booktitle = {EMNLP},
  year      = {2015}
}

@inproceedings{Williams2018ABC,
  title     = {A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference},
  author    = {Adina Williams and Nikita Nangia and Samuel R. Bowman},
  booktitle = {NAACL},
  year      = {2018}
}

@inproceedings{Dolan2005AutomaticallyCA,
  title     = {Automatically Constructing a Corpus of Sentential Paraphrases},
  author    = {William B. Dolan and Chris Brockett},
  booktitle = {IJCNLP},
  year      = {2005}
}

@inproceedings{Chu2017AnEC,
  title     = {An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation},
  author    = {Chenhui Chu and Raj Dabre and Sadao Kurohashi},
  booktitle = {ACL},
  year      = {2017}
}

@article{Freitag2016FastDA,
  title   = {Fast Domain Adaptation for Neural Machine Translation},
  author  = {Markus Freitag and Yaser Al-Onaizan},
  journal = {ArXiv},
  year    = {2016},
  volume  = {abs/1612.06897}
}

@inproceedings{Chu2018ASO,
  title     = {A Survey of Domain Adaptation for Neural Machine Translation},
  author    = {Chenhui Chu and Rui Wang},
  booktitle = {COLING},
  year      = {2018}
}

@inproceedings{Sang2003IntroductionTT,
  title     = {Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition},
  author    = {Erik Tjong Kim Sang and Fien De Meulder},
  booktitle = {CoNLL},
  year      = {2003}
}

@inproceedings{Rajpurkar2016SQuAD1Q,
  title     = {SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author    = {Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle = {EMNLP},
  year      = {2016}
}

@inproceedings{Zhang2018LanguageMT,
  title     = {Language Modeling Teaches You More than Translation Does: Lessons Learned Through Auxiliary Syntactic Task Analysis},
  author    = {Kelly W. Zhang and Samuel R. Bowman},
  booktitle = {BlackboxNLP@EMNLP},
  year      = {2018}
}

@inproceedings{Wang2019CanYT,
  title     = {Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling},
  author    = {Alex Wang and Jan Hula and Patrick Xia and R. Pappagari and R. Thomas McCoy and Roma Patel and Najoung Kim and Ian Tenney and Yinghui Huang and Katherin Yu and Shuning Jin and Berlin Chen and Benjamin Van Durme and Edouard Grave and Ellie Pavlick and Samuel R. Bowman},
  booktitle = {ACL},
  year      = {2019}
}

@inproceedings{Peters2018DeepCW,
  title     = {Deep Contextualized Word Representations},
  author    = {Matthew E. Peters and Mark Neumann and Mohit Iyyer and Matt Gardner and Christopher Clark and Kenton Lee and Luke Zettlemoyer},
  booktitle = {NAACL},
  year      = {2018}
}

@article{Zhu2020IncorporatingBI,
  title   = {Incorporating BERT into Neural Machine Translation},
  author  = {Jinhua Zhu and Yingce Xia and Lijun Wu and Di He and Tao Qin and Wen-gang Zhou and Houqiang Li and Tie-Yan Liu},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2002.06823}
}

@article{Lee2020MixoutER,
  title   = {Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models},
  author  = {Cheolhyoung Lee and Kyunghyun Cho and Wanmo Kang},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/1909.11299}
}

@inproceedings{luong2015stanford,
  title     = {Stanford Neural Machine Translation Systems for Spoken Language Domains},
  author    = {Minh-Thang Luong and Christopher D. Manning},
  booktitle = {IWSLT},
  year      = {2015}
}

@inproceedings{hildebrand2005adaptation,
  title     = {Adaptation of the translation model for statistical machine translation based on information retrieval},
  author    = {Almut Silja Hildebrand and Matthias Eck and Stephan Vogel and Alexander H. Waibel},
  booktitle = {EAMT},
  year      = {2005}
}

@inproceedings{biesialska2020continual,
  title     = {Continual Lifelong Learning in Natural Language Processing: A Survey},
  author    = {Biesialska, Magdalena  and
               Biesialska, Katarzyna  and
               Costa-juss{\`a}, Marta R.},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  month     = dec,
  year      = {2020},
  address   = {Barcelona, Spain (Online)},
  publisher = {International Committee on Computational Linguistics},
  url       = {https://aclanthology.org/2020.coling-main.574},
  doi       = {10.18653/v1/2020.coling-main.574},
  pages     = {6523--6541}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{ratford2019language,
  title  = {Language Models are Unsupervised Multitask Learners},
  author = {Alec Radford and Jeff Wu and R. Child and David Luan and Dario Amodei and Ilya Sutskever},
  year   = {2019}
}

@inproceedings{glorot2010understanding,
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  author    = {Xavier Glorot and Yoshua Bengio},
  booktitle = {AISTATS},
  year      = {2010}
}

@article{kumar2017onweight,
  author     = {Siddharth Krishna Kumar},
  title      = {On weight initialization in deep neural networks},
  journal    = {CoRR},
  volume     = {abs/1704.08863},
  year       = {2017},
  url        = {http://arxiv.org/abs/1704.08863},
  eprinttype = {arXiv},
  eprint     = {1704.08863},
  timestamp  = {Mon, 13 Aug 2018 16:48:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/Kumar17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{erhan2009thedifficulty,
  title     = {The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training},
  author    = {D. Erhan and Pierre-Antoine Manzagol and Yoshua Bengio and Samy Bengio and Pascal Vincent},
  booktitle = {AISTATS},
  year      = {2009}
}
@inproceedings{sutskever2014sequence,
  title     = {Sequence to Sequence Learning with Neural Networks},
  author    = {Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
  booktitle = {NIPS},
  year      = {2014}
}
@inproceedings{cho2014properties,
  title     = {On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches},
  author    = {Cho, Kyunghyun  and
               van Merri{\"e}nboer, Bart  and
               Bahdanau, Dzmitry  and
               Bengio, Yoshua},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  month     = oct,
  year      = {2014},
  address   = {Doha, Qatar},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W14-4012},
  doi       = {10.3115/v1/W14-4012},
  pages     = {103--111}
}

@inproceedings{bahdanau2015nmt,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{devlin2018bert,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author    = {J. Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle = {NAACL},
  year      = {2019}
}

@article{mccloskey1989catastrophic,
  title   = {Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem},
  author  = {M. McCloskey and N. Cohen},
  journal = {Psychology of Learning and Motivation},
  year    = {1989},
  volume  = {24},
  pages   = {109-165}
}

@article{yogatama2019learning,
  title   = {Learning and Evaluating General Linguistic Intelligence},
  author  = {Dani Yogatama and Cyprien de Masson d'Autume and Jerome T. Connor and Tom{\'a}s Kocisk{\'y} and Mike Chrzanowski and Lingpeng Kong and A. Lazaridou and Wang Ling and Lei Yu and Chris Dyer and P. Blunsom},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1901.11373}
}

@inproceedings{houlsby2019parameter,
  title        = {Parameter-efficient transfer learning for NLP},
  author       = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2790--2799},
  year         = {2019},
  organization = {PMLR}
}

@article{machavcek2018enriching,
  title     = {Enriching Neural MT through Multi-Task Training},
  author    = {Mach{\'a}{\v{c}}ek, Dominik},
  year      = {2018},
  publisher = {Univerzita Karlova, Matematicko-fyzik{\'a}ln{\'\i} fakulta}
}

@inproceedings{koehn2017six,
  title     = {Six Challenges for Neural Machine Translation},
  author    = {Koehn, Philipp  and
               Knowles, Rebecca},
  booktitle = {Proceedings of the First Workshop on Neural Machine Translation},
  month     = aug,
  year      = {2017},
  address   = {Vancouver},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/W17-3204},
  doi       = {10.18653/v1/W17-3204},
  pages     = {28--39},
  abstract  = {We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.}
}

@article{koehn2017nmt,
  author     = {Philipp Koehn},
  title      = {Neural Machine Translation},
  journal    = {CoRR},
  volume     = {abs/1709.07809},
  year       = {2017},
  url        = {http://arxiv.org/abs/1709.07809},
  eprinttype = {arXiv},
  eprint     = {1709.07809},
  timestamp  = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1709-07809.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{blunsom2013recurrent,
  title     = {Recurrent Continuous Translation Models},
  author    = {Kalchbrenner, Nal  and
               Blunsom, Phil},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  month     = oct,
  year      = {2013},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D13-1176},
  pages     = {1700--1709}
}

@inproceedings{bojar2015proceeding,
  title     = {Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal},
  author    = {Ondrej Bojar and Rajanya Chatterjee and Christian Federmann and Barry Haddow and Chris Hokamp and Matthias Huck and Varvara Logacheva and Pavel Pecina},
  booktitle = {WMT@EMNLP},
  year      = {2015}
}

@inproceedings{liu2020understanding,
  title     = {Understanding the Difficulty of Training Transformers},
  author    = {Liu, Liyuan  and
               Liu, Xiaodong  and
               Gao, Jianfeng  and
               Chen, Weizhu  and
               Han, Jiawei},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.463},
  doi       = {10.18653/v1/2020.emnlp-main.463},
  pages     = {5747--5763},
  abstract  = {Transformers have proved effective in many NLP tasks. However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively). Our objective here is to understand {\_}{\_}what complicates Transformer training{\_}{\_} from both empirical and theoretical perspectives. Our analysis reveals that unbalanced gradients are not the root cause of the instability of training. Instead, we identify an amplification effect that influences training substantially{---}for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output. Yet we observe that a light dependency limits the model potential and leads to inferior trained models. Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage{'}s training and unleash its full potential in the late stage. Extensive experiments show that Admin is more stable, converges faster, and leads to better performance}
}

@inproceedings{ruder2019transfer,
  title     = {Transfer Learning in Natural Language Processing},
  author    = {Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages     = {15--18},
  year      = {2019}
}

@inproceedings{bapna2019simple,
  title     = {Simple, Scalable Adaptation for Neural Machine Translation},
  author    = {Bapna, Ankur  and
               Firat, Orhan},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  month     = nov,
  year      = {2019},
  address   = {Hong Kong, China},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/D19-1165},
  doi       = {10.18653/v1/D19-1165},
  pages     = {1538--1548}
}

@inproceedings{han2021robust,
  title     = {Robust Transfer Learning with Pretrained Language Models through Adapters},
  author    = {Han, Wenjuan  and
               Pang, Bo  and
               Wu, Ying Nian},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  month     = aug,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.acl-short.108},
  doi       = {10.18653/v1/2021.acl-short.108},
  pages     = {854--861},
  abstract  = {Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.}
}

@article{delange2021continual,
  author  = {Delange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ales and Slabaugh, Greg and Tuytelaars, Tinne},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title   = {A continual learning survey: Defying forgetting in classification tasks},
  year    = {2021},
  volume  = {},
  number  = {},
  pages   = {1-1},
  doi     = {10.1109/TPAMI.2021.3057446}
}

@inproceedings{panayotov2015librispeech,
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  title     = {Librispeech: An ASR corpus based on public domain audio books},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {5206-5210},
  doi       = {10.1109/ICASSP.2015.7178964}
}

@inproceedings{pfeiffer2020madx,
  title     = {{MAD-X}: {A}n {A}dapter-{B}ased {F}ramework for {M}ulti-{T}ask {C}ross-{L}ingual {T}ransfer},
  author    = {Pfeiffer, Jonas  and
               Vuli{\'c}, Ivan  and
               Gurevych, Iryna  and
               Ruder, Sebastian},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  month     = nov,
  year      = {2020},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2020.emnlp-main.617},
  doi       = {10.18653/v1/2020.emnlp-main.617},
  pages     = {7654--7673}
}

@article{ruckle2020adapterdrop,
  title   = {AdapterDrop: On the Efficiency of Adapters in Transformers},
  author  = {Andreas R{\"u}ckl{\'e} and Gregor Geigle and Max Glockner and Tilman Beck and Jonas Pfeiffer and N. Reimers and Iryna Gurevych},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2010.11918}
}

@article{canese2013pubmed,
  title     = {PubMed: the bibliographic database},
  author    = {Canese, Kathi and Weis, Sarah},
  journal   = {The NCBI Handbook},
  volume    = {2},
  pages     = {1},
  year      = {2013},
  publisher = {National Center for Biotechnology Information (US)}
}

@inproceedings{stolcke1998dialog,
  title     = {Dialog act modeling for conversational speech},
  author    = {Stolcke, Andreas and Shriberg, Elizabeth and Bates, Rebecca and Coccaro, Noah and Jurafsky, Daniel and Martin, Rachel and Meteer, Marie and Ries, Klaus and Taylor, Paul and Van Ess-Dykema, Carol and others},
  booktitle = {AAAI Spring Symposium on Applying Machine Learning to Discourse Processing},
  pages     = {98--105},
  year      = {1998}
}

@article{ii2020developingrm,
  title   = {Developing RNN-T Models Surpassing High-Performance Hybrid Models with Customization Capability},
  author  = {Jinyu Li and Rui Zhao and Zhong Meng and Yanqing Liu and Wenning Wei and S. Parthasarathy and Vadim Mazalov and Z. Wang and Lei He and Sheng Zhao and Y. Gong},
  journal = {INTERSPEECH 2020},
  year    = {2020},
  volume  = {abs/2007.15188}
}

@inproceedings{vaswani2017attention,
  title     = {Attention is all you need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {Advances in neural information processing systems},
  pages     = {5998--6008},
  year      = {2017}
}

@inproceedings{pfeiffer2021adapterfusion,
  title     = {{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning},
  author    = {Pfeiffer, Jonas  and
               Kamath, Aishwarya  and
               R{\"u}ckl{\'e}, Andreas  and
               Cho, Kyunghyun  and
               Gurevych, Iryna},
  booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
  month     = apr,
  year      = {2021},
  address   = {Online},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2021.eacl-main.39},
  pages     = {487--503}
}

@misc{winata2020adapt,
  title         = {Adapt-and-Adjust: Overcoming the Long-Tail Problem of Multilingual Speech Recognition},
  author        = {Genta Indra Winata and Guangsen Wang and Caiming Xiong and Steven Hoi},
  year          = {2020},
  eprint        = {2012.01687},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL}
}

@inproceedings{lee2021adaptable,
  title        = {Adaptable multi-domain language model for transformer asr},
  author       = {Lee, Taewoo and Lee, Min-Joong and Kang, Tae Gyoon and Jung, Seokyeoung and Kwon, Minseok and Hong, Yeona and Lee, Jungin and Woo, Kyoung-Gu and Kim, Ho-Gyeong and Jeong, Jiseung and others},
  booktitle    = {ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages        = {7358--7362},
  year         = {2021},
  organization = {IEEE}
}

@inproceedings{pham2020study,
  title     = {A study of residual adapters for multi-domain neural machine translation},
  author    = {Pham, Minh Quang and Crego, Josep M and Yvon, Fran{\c{c}}ois and Senellart, Jean},
  booktitle = {Proceedings of the Fifth Conference on Machine Translation},
  pages     = {617--628},
  year      = {2020}
}

@article{french1999catastrophic,
  title     = {Catastrophic forgetting in connectionist networks},
  author    = {French, Robert M},
  journal   = {Trends in cognitive sciences},
  volume    = {3},
  number    = {4},
  pages     = {128--135},
  year      = {1999},
  publisher = {Elsevier}
}

@article{rosenfeld2018incremental,
  title     = {Incremental learning through deep adaptation},
  author    = {Rosenfeld, Amir and Tsotsos, John K},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  volume    = {42},
  number    = {3},
  pages     = {651--663},
  year      = {2018},
  publisher = {IEEE}
}

@article{kirkpatrick2017overcoming,
  title     = {Overcoming catastrophic forgetting in neural networks},
  author    = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal   = {Proceedings of the national academy of sciences},
  volume    = {114},
  number    = {13},
  pages     = {3521--3526},
  year      = {2017},
  publisher = {National Acad Sciences}
}

@incollection{thrun1998lifelong,
  title     = {Lifelong learning algorithms},
  author    = {Thrun, Sebastian},
  booktitle = {Learning to learn},
  pages     = {181--209},
  year      = {1998},
  publisher = {Springer}
}

@article{rochan2021unsupervised,
  title   = {Unsupervised Domain Adaptation in LiDAR Semantic Segmentation with Self-Supervision and Gated Adapters},
  author  = {Rochan, Mrigank and Aich, Shubhra and Corral-Soto, Eduardo R and Nabatchian, Amir and Liu, Bingbing},
  journal = {arXiv preprint arXiv:2107.09783},
  year    = {2021}
}

@article{kudo2018sentencepiece,
  author     = {Taku Kudo and
                John Richardson},
  title      = {SentencePiece: {A} simple and language independent subword tokenizer
                and detokenizer for Neural Text Processing},
  journal    = {CoRR},
  volume     = {abs/1808.06226},
  year       = {2018},
  url        = {http://arxiv.org/abs/1808.06226},
  eprinttype = {arXiv},
  eprint     = {1808.06226},
  timestamp  = {Sun, 02 Sep 2018 15:01:56 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1808-06226.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{agarap2018deep,
  title   = {Deep learning using rectified linear units (relu)},
  author  = {Agarap, Abien Fred},
  journal = {arXiv preprint arXiv:1803.08375},
  year    = {2018}
}

@article{lei2016layernorm,
  author     = {Lei Jimmy Ba and
                Jamie Ryan Kiros and
                Geoffrey E. Hinton},
  title      = {Layer Normalization},
  journal    = {CoRR},
  volume     = {abs/1607.06450},
  year       = {2016},
  url        = {http://arxiv.org/abs/1607.06450},
  eprinttype = {arXiv},
  eprint     = {1607.06450},
  timestamp  = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

 @misc{johnson_2022,
  title   = {Seq2seq (sequence to sequence) model with pytorch},
  url     = {https://www.guru99.com/seq2seq-model.html},
  journal = {Guru99},
  author  = {Johnson, Daniel},
  year    = {2022},
  month   = {May}
}

 @misc{seq2seq_and_attention_2022,
  title   = {Sequence to sequence (seq2seq) and attention},
  url     = {https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html},
  journal = {Seq2seq and Attention},
  author  = {Voita, Lena},
  year    = {2022},
  month   = {Apr}
} 