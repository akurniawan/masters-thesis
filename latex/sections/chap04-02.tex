% Target: 35 pages
% Current: 3

\chapter{Adapters Effectiveness in Machine Translation}
\label{chap:adaptefct}
We continue the study from the previous chapter to understand more about the relation between adapters and pre-trained models. Similar to the previous chapter, we use BERT and its variants as the pre-trained models and fine-tune them with adapters. This study aims to evaluate the combination of adapters and BERT in machine translation and study the effectiveness of adapters by putting them only in the encoder or the decoder. We also experiment with down-scaling the pre-trained model size and try to recover the performance of the full-sized model. We separate the experiments into three different areas:
\begin{itemize}
    \item Use BERT weights\footnote{We use publicly available BERT model from Huggingface hub \url{https://huggingface.co}} as the pre-trained weights and investigate the importance of adapters in encoder or decoder.
    \item Use BERT weights and investigate their importance compared to random weights in the encoder or decoder while fine-tuning with adapters.
    \item Down-scaling BERT weights by either zeroing out half of BERT's weights (\texttt{zbert}) or completely removing them from the weight matrices, squashing the matrices (\texttt{zsbert}). We use the down-scaling technique to understand whether we can use adapters to recover the performance of the original BERT (without adapters) while using fewer parameters.
\end{itemize}

\section{Fixed Variable Parameters of Experients}
\subsection{Framework}
% \begin{table}[t]
%     \centering
%     \begin{tabular}{@{}cc@{}}
%         \toprule
%         \textbf{Name}            & \textbf{Value}        \\ \midrule
%         \textbf{Batch size}      & 64                    \\
%         \textbf{Learning rate}   & 0.0005                \\
%         \textbf{Vocabulary size} & 31102 (de), 30522(en) \\ \bottomrule
%     \end{tabular}
%     \caption{Fixed hyperparameters throughout the experiments}
%     \label{tab:hyp_invest}
% \end{table}

As we mentioned at the beginning of the chapter, we have several scenarios we use to conduct the experiments. We start by describing the variables that we fixed throughout the experiments. As we have mentioned in \cref{chap:03}, we use Huggingface as our main framework with added modifications for adapters. Contrary to \cref{chap:adaptmt}, we do not investigate language models that we train ourselves, but instead, we focus only on the BERT language model.

The model and hyperparameters that we use throughout the experiment remain the same as described in \cref{chap:adaptmt}. We use transformer model with seq2seq architecture and the BERT-based hyperparameter configuration to initialize both the encoder and the decoder.

\subsection{Dataset}
As mentioned in the previous section, our focus in this chapter is on machine translation. We use IWSLT dataset to perform the fine-tuning as well as the evaluation for the models.

\section{Original BERT}
\subsection{Size of Adapters}
\subsubsection{Experiment Setup and Motivation}
\paragraph{}
In these experiments, we freeze both the encoder and decoder and modify the reduction ratio parameter in the adapters. The adapter serves as a bottleneck layer with two dense layers and a non-linear function between them. The reduction ratio is defined as the fraction of the original representation dimension divided by the adapter vector size. For instance, if we use 16 as the reduction ratio, we reduce the original layers by 16 with the first dense layer and then scale it back to the original size with the second dense layer.

We try out various sizes of reduction ratios to compare the results. This reduction aims to see whether we can further benefit from enlarging the adapters' bottleneck size. We use 16, 8, 4, 2, and 1 as the ratio values for this experiment. We compare the results with the baseline BERT that we fine-tuned by only training the cross-attention and output layers. We will refer to this baseline as \texttt{baseline\_bert} for the entirety of this chapter.

\subsubsection{Experiment Results}
In this section, we compare the results of the \texttt{baseline\_bert} with BERT models that are fine-tuned with adapters in different reduction ratios. We are fine-tuning the cross-attention and output layers for the \texttt{baseline\_bert} model and freeze the rest of the model. We can see in \cref{img:adapt_bert_ratio} that even the smallest model (\texttt{adapt\_bert\_reduc\_16}) can already outperform the baseline by around 2 BLEU points. This shows that the adapters can help improve the model's performance by adding only a small number of weights during the fine-tuning.

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/adapter_bert_baseline_adapters.png}}
    \centering
    \caption{Comparison between baseline BERT model and adapters model with different ratio (16, 8, 4, 2, 1).}
    \label{img:adapt_bert_ratio}
\end{figure}

Despite better performance than the baseline model, the difference between the ratios is minimal. It suggests that there is not much benefit in expanding the size of adapters for the normal size BERT. It is possible that it is no longer trivial to simply append larger-size adapters for fine-tuning the model and getting better performance. Further changes may be required to handle the different nature of BERT's output as it is naturally different from the common auto-regressive machine translation objective. Furthermore, we recall that we also have problems where the models struggle in generating good translations when the input contains an unavailable token such as \texttt{\&quot\;}.

\subsection{Position of Adapters (Encoder vs Decoder)}
\label{sec:posada}
\subsubsection{Experiment Setup and Motivation}
We would like to see the importance of adapters when they are put in different places. Since we are working with seq2seq architecture in this work, we would like to see whether only incorporating adapters on either the encoder or the decoder can already be beneficial and reduce the number of parameters added to the model.

\subsubsection{Experiment Result}
\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/bert_pos.png}}
    \centering
    \caption[Results of ablation study for adapters in the encoder or the decoder.]{Comparison between baseline BERT model and adapters model where the adapters are placed in three different setups: 1) Adapters in both encoder and decoder (\texttt{adapt\_bert\_reduc\_16}); 2) Adapters only in encoder (\texttt{adapter\_bert\_bert}); 3) Adapters only in decoder (\texttt{bert\_adapter\_bert}).}
    \label{img:adapt_bert_pos}
\end{figure}
We see in \cref{img:adapt_bert_pos} that adding adapters just in the encoder brings an improvement and outperforms the baseline. Adapters only in the encoder train the fastest at the beginning, and their final performance is almost the same as if we added the adapters on both sides. For the decoder, on the other hand, we can see that aside from a more promising start, there is no benefit as there is no improvement in terms of late BLEU scores compared to the baseline. With this finding, we can reduce the cost of fine-tuning further by half when we do not include the adapters on the decoder.
% Adding the adapters on the encoder and fine-tuning it is more cost-effective than the decoder. With this finding, we can reduce the cost of fine-tuning by half when we do not include the adapters on the decoder.

\subsection{True BERT in Encoder vs Decoder}
\label{sec:pospre}
\subsubsection{Experiment Setup and Motivation}
This section investigates the importance of pre-trained BERT in the encoder or the decoder when the adapters are used for fine-tuning the models.
% In addition to that, we also expand the experiment further by understanding the correlation of adding adapters on top of the randomly set weights.

% We start by describing the setup in this experiment. 
The previous chapter introduced an experiment where we instantiate the base transformer model with only random fixed weights. We are then fine-tuning the base transformer model by only updating the cross-attention, adapters, and output layers. In this setup, we are doing experiments in a similar concept. We use random (fixed) weights instead of the original pre-trained BERT weights in the encoder or the decoder. We thus have a seq2seq model with the random weights encoder followed by the BERT decoder and vice versa. In the fine-tuning stage, we update the adapters in the encoder and decoder, the cross-attention (or cross-attention layer only in our baseline models), and the output layers.

The purposes of the experiments are:
\begin{itemize}
    \item We want to understand further the importance of the pre-training model when fine-tuning with adapters. By initializing the models with BERT only in one component, we can see whether it is necessary to use BERT on both components when adapters are incorporated.
    \item We want to understand the capability of adapters when either one of the components does not contain useful information (relative to BERT). We would like to see whether the adapters can recover or even outperform some of the performance that we have already gathered from the previous chapters and sections.
\end{itemize}

\subsubsection{Experiment Results}
This section compares models that use adapters in either or both the encoder and decoder while only initializing one of these components with pre-trained BERT and the other one with (fixed) random weights.

\paragraph{Randomly Set Weights on Encoder}
In this part of the section, we want to answer the main question: ``To what extent can the adapters restore the missing gap when the encoder does not contain useful information (relative to BERT)?''

We can see from \cref{img:adapt_bert_randenc} that when adapters are used in both components to a model with random encoder weights (\texttt{adapter\_randenc\_adapter\_bert}), we get to almost 20 BLEU points. This is relatively higher than the other two setups: adapters only in the encoder (\texttt{adapter\_randenc\_bert}) and only in the decoder (\texttt{randenc\_adapter\_bert}). However, compared to the baseline, we are missing 4 BLEU points when we set the encoder with completely random weights. This means that the base encoder model did contain relatively essential information that the adapters can not simply restore during the fine-tuning.

\begin{figure}[t]
    {\includegraphics[width=0.85\textwidth]{img/adapter_bert_randenc.png}}
    \centering
    \caption[Random + BERT: Comparison for model with adapters in the decoder and the encoder is initalized with random weights.]{Random + BERT: Comparison between baseline BERT model and adapters model where the adapters are placed in three different setups: 1) Adapters in both encoder and decoder (\texttt{adapt\_bert\_reduc\_16}); 2) Adapters only in encoder (\texttt{adapter\_bert\_bert}); 3) Adapters only in decoder (\texttt{bert\_adapter\_bert}) and the decoder is initialized with BERT while the encoder is initialized with random numbers.}
    \label{img:adapt_bert_randenc}
\end{figure}

% We further focus on the adapters' performance compared to the baseline model that is only fine-tuned on the cross-attention layer to see whether fine-tuning the cross-attention is already enough or if there is any benefit in adding adapters. We can see that the model that only fine-tuning the cross-attention layer (\texttt{baseline\_bert}) can not learn at all, while the adapters can perform significantly better. This marks the capability of the adapter when faced with a randomly set encoder.

When the adapters are removed from the decoder (\texttt{adapter\_randenc\_bert}), we see a degradation in performance about 1 BLEU point compared to the model that uses adapters on both side (\texttt{adapter\_randenc\_adapter\_bert}). However, when the adapters are removed from the encoder (\texttt{randenc\_adapter\_bert}), the performance is completely depleted to zero during the training. We also see the same behaviour in the next section when the weights on the decoder are set randomly. This tells us that it is not trivial to simply fine-tune the cross-attention without further modifying the encoder's parameters when the parameters on the encoder are completely random.

\paragraph{Randomly Set Weights on Decoder}
Similar to the previous section, the main question in this experiment is, ``To what extent can the adapters restore the performance when the decoder does not contain useful information (relative to BERT)?''

In contrast to when the randomly set weights are on the encoder side, we can see from \cref{img:adapt_bert_randdec} that fixing a random decoder leads to performance comparable to the one we have on \texttt{bert\_baseline}. This tells us that the pre-trained weights in the encoder are more important than in the decoder when we have adapters on both sides. However, when removing the adapters on the encoder, we see similar performance as in the previous section, where the performance drops to zero in the middle of training. This further strengthens our argument that adapters are necessary to adjust the weights in the model so that the cross-attention layer can work properly.

\begin{figure}[h]
    {\includegraphics[width=0.85\textwidth]{img/adapter_bert_randdec.png}}
    \centering
    \caption[BERT + Random: Comparison for model with adapters in the decoder and the decoder is initalized with random weights]{BERT + Random: Comparison between baseline BERT model and adapters model where the adapters are placed in three different setups: 1) Adapters in both encoder and decoder (\texttt{adapt\_bert\_reduc\_16}); 2) Adapters only in encoder (\texttt{adapter\_bert\_bert}); 3) Adapters only in decoder (\texttt{bert\_adapter\_bert}) and the encoder is initialized with BERT while the decoder is initialized with random numbers.}
    \label{img:adapt_bert_randdec}
\end{figure}

On the other hand, when we remove the adapters from the decoder side, we can see that the performance is not as bad as when the adapters are removed from the encoder, but we still see a reduction in performance. We see a reduction around less than 1 BLEU point when the model reaches 400k steps in the training stage. It is possible that even with random weights on the decoder side, the adapters help the cross-attention layers produce a good vector representation with meaningful features for the decoder to generate reasonable translation outputs.

\section{BERT Size Reduction}
\subsection{Zeroing Columns}
\subsubsection{Experiment Setup and Motivation}
In this experiment, we will focus on the soft reduction of BERT weights by zeroing the weight matrices on every even column and row indices within the transformer body as well as in the embedding. We load the pre-trained BERT weights, manually edit them and then continue the experiments by fine-tuning the cross-attention, adapters, and output layers. We refer to this setup as \texttt{zbert} for the rest of this chapter.

Besides removing the columns, we also perform experiments where we put the adapters either on the encoder or the decoder. This particular experiment aims to understand the model's behaviour when the pre-trained BERT is replaced with this particular setup.

\subsubsection{Comparison with BERT Baseline (Full BERT Fine-tuning)}
We first compare the \texttt{zbert} model without adapters and only fine-tune the cross-attention and output layers. We use \texttt{zbert} weights on both the encoder and decoder so that it is comparable to the model that uses full-weight BERT. We use the full-weight BERT as the baseline in this experiment.

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/baseline_zbert.png}}
    \centering
    \caption{Comparison between baseline BERT model and baseline \texttt{zbert} models.}
    \label{img:baseline_zbert}
\end{figure}

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/adapter_zbert.png}}
    \centering
    \caption{Comparison between baseline BERT model, baseline \texttt{zbert} and adapters \texttt{zbert} models.}
    \label{img:adapter_zbert}
\end{figure}

We can see in \cref{img:baseline_zbert} that we are losing performance of about 4 BLEU points. This is significant as we lose essential features from the original BERT model. To see whether we can recover some of the performance with adapters, we continue our experiment by fine-tuning the \texttt{zbert} model that is instantiated on both encoder and decoder sides with adapters. We can see from \cref{img:adapter_zbert} that we only managed to recover 1 BLEU point with a reduction ratio of 16.

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/adapter_zbert_ratio.png}}
    \centering
    \caption{Comparison between baseline BERT model and different reduction ratio of \texttt{zbert} models.}
    \label{img:adapter_zbert_ratio}
\end{figure}

From \cref{img:adapter_zbert_ratio}, when we increase the size of the reduction ratio, initially, we can see some improvement in the BLEU score compared to the higher ratio. However, they eventually converge to a similar performance by the end of training with no significant difference between different ratios. From this result, we can understand that depending on the base pre-trained model, adapters still have a limitation in achieving certain performance.

\paragraph{Adapters Position}
In this section, we aim to understand whether the position of both adapters and the pre-trained models affect the model's performance, similar to what we have seen in \cref{sec:posada}. We use a similar setup as in the previous section, with the exception that we use \texttt{zbert} as the pre-trained model instead of the original BERT model.

\begin{figure}[h]
    {\includegraphics[width=0.85\textwidth]{img/zbert_pos.png}}
    \centering
    \caption[Comparison between baseline BERT and \texttt{zbert} models.]{Comparison between baseline BERT model, baseline \texttt{zbert} model, adapters in both encoder and decoder of \texttt{zbert} model (\texttt{adapt\_zbert\_reduc\_16}), adapters only in encoder of \texttt{zbert} model (\texttt{adapter\_zbert\_zbert}), and adapters only in decoder of \texttt{zbert} model (\texttt{zbert\_adapter\_zbert}).}
    \label{img:zbert_pos}
\end{figure}

We can see from \cref{img:zbert_pos} that when we include adapters on both the encoder and decoder, we can outperform the baseline \texttt{zbert} in around 2 BLEU points. This shows that, similar to the models that use BERT as the pre-trained model, the adapters can help to improve the performance further, even though some of the information is already missing in the base model.

Furthermore, we can also see that, similar to the BERT model experiments, fine-tuning with adapters only on the encoder side (\texttt{adapter\_zbert\_zbert}) performs much better than on the decoder side (\texttt{zbert\_adapter\_zbert}). Other than that, we can also see that incorporating adapters only on the encoder side helps the model achieve better performance faster than using adapters on both sides. This further supports our hypothesis that updating the representation on the encoder side is more beneficial. Additionally, we can also see the same behaviour as the original pre-trained BERT that \texttt{zbert\_adapter\_zbert} performance is close to the baseline model (\texttt{baseline\_zbert}), where we are only fine-tuning the cross-attention and output layers. This could mean that fine-tuning the decoder may not be enough to achieve better performance when the representation from the source side is unchanged.

\subsection{Model Down-Scaling}
\subsubsection{Experiment Setup and Motivation}
This experiment is the follow-up from \texttt{zbert}, where we zeroed out half of the elements in the matrices. More specifically, we are completely removing those elements from the matrix instead of just zeroing out the elements. The way we do this is similar to the one we do on \texttt{zsbert}. We remove the matrix elements on every even column and row in the transformer body and the embedding. We again do the weights processing offline before using it as the pre-trained model. For the rest of this writing, we refer to this setup as \texttt{zsbert}.

Furthermore, we also follow a similar setup as in \cref{sec:posada} where we experiment with the position of the adapters. The goal of this experiment is to understand the behaviour of this model compared to the baseline as well as \texttt{zbert}.

\subsubsection{Comparison with BERT Baseline and \texttt{zbert}}
\label{sec:compbasezbertzsbert}

\begin{figure}[t]
    {\includegraphics[width=0.85\textwidth]{img/baseline_zsbert.png}}
    \centering
    \caption{Comparison between baseline BERT model and baseline \texttt{zsbert} model.}
    \label{img:baseline_zsbert}
\end{figure}

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/adapter_zsbert.png}}
    \centering
    \caption{Comparison between baseline BERT model, baseline \texttt{zbert}, baseline \texttt{zsbert} and adapters \texttt{zsbert} models.}
    \label{img:adapter_zsbert}
\end{figure}


We begin by comparing \texttt{zsbert} with the BERT baseline. We see in \cref{img:baseline_zsbert} that the performance degrades by more than 10 BLEU points. This is also significantly worse than \texttt{zbert}, where we only lose 7.5 BLEU points. Our initial hypothesis was that \texttt{zsbert}'s performance should be comparable to \texttt{zbert} as we are fundamentally performing the same reduction technique. After some investigation, we realized that removing the weights from the network and zeroing out the matrices are not directly comparable because we also need to consider the computation of layer normalization, which highly depends on the matrix dimension. We found this discrepancy by performing a manual evaluation where we used an arbitrary vector as the input to the network and monitored the output in each of the network layers. We found a slight difference in the layer's output between the zeroed and completely removed weights. Even though the difference is minimal, the output discrepancy gets propagated to the top layers, causing the final network output to differ significantly.

Next, we study the interplay of model down-scaling and adapters. We can see in \cref{img:adapter_zsbert} that \texttt{zsbert} with a 16 ratio adapters manages to improve the performance up to 6 BLEU points compared to \texttt{zbert} without adapters. This shows that adapters can still improve the model's performance even when some weights are missing. Furthermore, despite still showing difficulties in reaching the baseline performance, \cref{img:adapter_zsbert_ratio} shows that we can still improve the model's performance by reducing the adapters reduction ratio.
We can see that the model with the lowest reduction ratio (1) manages to close the performance gap significantly with the baseline model. This is one of our prominent results because we can see from \cref{tab:numvars} that the total number of weights (including adapters) required to fine-tune the model is significantly lower than the original BERT model.
% We also notice a leap in final performance when we compare the adapter model with an equal reduction ratio (16) between \texttt{zbert} and \texttt{zsbert}. We can see that initially \texttt{zsbert} performs worse than \texttt{zbert}. After some steps, we can see the performance in \texttt{zbert} starting to stall but not in \texttt{zsbert}. We hypothesize that this relates to a similar reason that we stated in the original BERT model, where we could not see any improvement when increasing the reduction ratio. It is possible that when we reduce the size of the original pre-trained model, the adapters manage to adjust the flow of information within the network and better replace the missing information with new knowledge that is more important for solving the task.

\begin{figure}[]
    {\includegraphics[width=0.85\textwidth]{img/adapter_zsbert_ratio.png}}
    \centering
    \caption{Comparison between baseline BERT model and different reduction ratio of \texttt{zsbert} models.}
    \label{img:adapter_zsbert_ratio}
\end{figure}


\begin{table*}[]
    \centering
    \begin{tabular}{@{}|l|r|r|r|r|@{}}
        \toprule
        \multicolumn{1}{|c|}{\textbf{Name}}                                                              &
        \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\# Trainable\\  Variables\end{tabular}}}  &
        \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\# Untrainable\\ Variables\end{tabular}}} &
        \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}\# Total\\ Variables\end{tabular}}}       &
        \multicolumn{1}{c|}{\textbf{\begin{tabular}[c]{@{}c@{}}Percentage\\ Trainable\end{tabular}}}                                                      \\ \midrule
        \textbf{Ratio 16}                                                                                & 7.736.826  & 95.143.296 & 102.880.122 & 7.5\%  \\
        \textbf{Ratio 8}                                                                                 & 8.179.770  & 95.143.296 & 103.323.066 & 7.9\%  \\
        \textbf{Ratio 4}                                                                                 & 9.065.658  & 95.143.296 & 104.208.954 & 8.7\%  \\
        \textbf{Ratio 2}                                                                                 & 10.837.434 & 95.143.296 & 105.980.730 & 10.2\% \\
        \textbf{Ratio 1}                                                                                 & 14.380.986 & 95.143.296 & 109.524.282 & 13.1\% \\ \bottomrule
    \end{tabular}
    \caption{Total trainable variables in \texttt{zsbert} with adapters on different ratio vs normal BERT model}
    \label{tab:numvars}
\end{table*}

\subsubsection{Adapters Position}

\begin{figure}[h]
    {\includegraphics[width=0.85\textwidth]{img/zsbert_pos.png}}
    \centering
    \caption[Comparison between baseline BERT and \texttt{zsbert} models.]{Comparison between baseline BERT model, baseline \texttt{zsbert} model, adapters in both encoder and decoder of \texttt{zsbert} model (\texttt{adapt\_zsbert\_reduc\_16}), adapters only in encoder of \texttt{zsbert} model (\texttt{adapter\_zsbert\_zsbert}), and adapters only in decoder of \texttt{zsbert} model (\texttt{zsbert\_adapter\_zsbert}).}
    \label{img:zsbert_pos}
\end{figure}

From \cref{img:zsbert_pos}, similar to the \texttt{zbert} experiments, we can see similar behaviour where models fine-tuned with adapters outperform the baseline \texttt{zsbert} and \texttt{zbert} models. However, compared to \texttt{zbert} experiments, we notice a bigger improvement in \texttt{zsbert}'s final performance. In \texttt{zbert}, the difference between baseline and adapters is within 5 BLEU points. On the other hand, in \texttt{zsbert}, we see the improvement is within 8 BLEU points. This result is particularly interesting for us as we expect the difference to be similar to \texttt{zbert}. We recall from \ref{sec:compbasezbertzsbert} that this is due to the numerical error from the layer normalization, i.e. a difference in constant used to perform vector normalization in the layer normalization. In other words, we observed a worse performance in \texttt{zbert} than we got for \texttt{zsbert} with adapters.

We deep-dive further in \cref{img:zbert_vs_zsbert} to show the comparison between adapters in \texttt{zbert} and \texttt{zsbert}. We use a reduction ratio of 16 to compare the adapter's performance between these two setups. We notice a leap in the final performance when we compare the adapter model with an equal reduction ratio (16) between \texttt{zbert} and \texttt{zsbert}. We can see that initially \texttt{zsbert} performs worse than \texttt{zbert}. After some steps, we can see the performance in \texttt{zbert} starting to stall but not in \texttt{zsbert}. We hypothesize that this relates to a similar reason we stated in the original BERT model, where we could not see any improvement when increasing the reduction ratio. It is possible that when we reduce the original pre-trained model's size, the adapters adjust the flow of information within the network and better replace the missing information with new knowledge that is more important for solving the task. Another possibility is simply because \texttt{zbert} is a "heavier model" than \texttt{zsbert} as it contains more parameters and thus has a harder time for the adapters to recover.

\begin{figure}[t]
    {\includegraphics[width=0.85\textwidth]{img/zbert_vs_zsbert.png}}
    \centering
    \caption{Comparison adapters performance in \texttt{zsbert} and \texttt{zbert}. Both are using reduction ratio 16 and the adapters are placed on encoder and decoder.}
    \label{img:zbert_vs_zsbert}
\end{figure}

We see a similar behaviour as in \texttt{zbert} experiments concerning the position of the adapters. In \cref{img:zsbert_pos}, the benefit of incorporating adapters on the encoder side is apparent and outperforms the decoder counterpart. We can also see a similar behaviour where the model's performance with adapters on the encoder eventually outperforms the model with adapters on both sides. Furthermore, we also see a similar behaviour as in \texttt{zbert} for models with adapters in the decoder only where the performance is very close to the baseline and not improving as much as on the encoder side. We hypothesize that the same reason as we have stated in \texttt{zbert} could apply in \texttt{zsbert} as well. Essentially, we need to modify the representation on the encoder side in order to achieve better performance.