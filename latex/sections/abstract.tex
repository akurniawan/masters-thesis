
Fine-tuning in deep learning have been gaining popularity in the past couple of years due to the
capabilities in leveraging past information into new tasks. However, there still some difficulties
in properly fine-tuning models to achieve state-of-the-art performance. Adapter modules are
proposed as an alternative to alleviate the aforementioned problem by injecting a small module
between pre-trained model layers such as BERT.
