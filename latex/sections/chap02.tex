\chapter{Related Work}

This chapter reviews the previous works related to this thesis on the adaptation of transformer and BERT with adapters in machine translation. Specifically, in Section \ref{sec:prelm_mt}, we review works in incorporating BERT for machine translation. In section \ref{sec:adapter_seq}, we review the usage of adapters using transformer as the base model in sequence to sequence models in various fields such as NLU, Automatic Speech Recognition (ASR), and MT.

\section{Pre-training Language Models in Machine Translation}
\label{sec:prelm_mt}

This section will discuss three different works that try to incorporate BERT into NMT. The goals of these works are different from the works proposed in this thesis as they are primarily leveraging the feature representation of BERT instead of using BERT directly to fine-tune in NMT.

\cite{weng2020acquiring} proposes to acquire knowledge from pre-trained models such as BERT with a framework called $A_{PT}$. $A_{PT}$ consisted of two different modules with different goals. The first goal is to learn a task-specific representation through adaptation from general representation in the pre-trained models and two controlling methods to control the task-specific representation into NMT. They propose to achieve the first goal by using a dynamic fusion mechanism. They claim this method could provide rich contextual information to better model sentences in NMT. An illustration of their proposed approach can be seen in Figure \ref{img:dyn_fn}. The second goal is to introduce a knowledge distillation to prune the knowledge from pre-trained models. They claim this method could help NMT learn essential knowledge about translation from source sentence to target sentence and help generate better translation. Moreover, based on their empirical results, they achieve optimal results when both the methods are applied in the encoder and decoder.

\begin{figure}[h]
    {\includegraphics[width=0.95\textwidth]{img/dynamic_fusion.png}}
    \centering
    \caption{Illustration of dynamic fusion mechanism. Figure reprinted from \cite{weng2020acquiring}.}
    \label{img:dyn_fn}
\end{figure}

\cite{yang2020towards} try to combat issues such as requiring a long time to train NMT models and catastrophic forgetting problem from updating too many of the pre-trained models in the fine-tuning process. They propose a concerted training approach ($CT_{NMT}$) that consists of three different techniques: 1) Asymptotic distillation; 2) Dynamic switch for knowledge fusion; 3) Rate-scheduled updating. The first technique is used to keep the pre-training knowledge intact. They achieve this by using the pre-trained BERT as a teacher network and the encoder of the NMT models as the student. The goal of the first technique is to mimic the representation coming from the pre-trained BERT by minimizing the cross-entropy loss. The second technique is introduced to perform a combination between the representation from BERT and the encoder of NMT. They achieve this by using a gating mechanism that uses the source as the input to decide how to fuse the representations. The intuition of this technique is that for a particular sentence, BERT might provide a better representation than the currently trained model. Using this gating mechanism, they try to adjust the use of BERT representation in the encoder dynamically. The intuition of this gating mechanism is that BERT might better encode some sentences, and some might not. The last technique uses a scheduling policy for adjusting the learning rate. They propose using this as they found that updating BERT LM and the NMT at different paces would benefit the final model. An illustration of the asymptotic distillation and dynamic switch mechanism is depicted in Figure \ref{img:ctnmt}.

\begin{figure}[h]
    {\includegraphics[width=0.95\textwidth]{img/ctnmt.png}}
    \centering
    \caption{Illustration of asymptotic distillation and dynamic switch. Figure reprinted from \cite{yang2020towards}.}
    \label{img:ctnmt}
\end{figure}

\cite{chen2019distilling} sees the discrepancy between the objective used to pre-train BERT and the objective used in common NLG tasks such as MT. They propose to introduce knowledge distillation approach learned in BERT for text generation tasks. The first proposal introduces a new objective called Conditional Masked Language Modeling (C-MLM). This objective is inspired by MLM but induces another conditional constraint while fine-tuning the pre-trained BERT on a target dataset. The second approach consisted of leveraging the fine-tuned BERT as a teacher model and using the logits output as the learning target for the student network to mimic. They claim that the proposed approach improves the generation output by leveraging exploitation that encourages conventional text generation models to plan. They also claim that BERT's ability to look forward into the future can act as an effective regularizer that can help to boost the generation output performance.

\begin{figure}[h]
    {\includegraphics[width=0.95\textwidth]{img/bert_distill.png}}
    \centering
    \caption{Illustration of seq2seq architecture. Figure reprinted from \cite{chen2019distilling}.}
    \label{img:bert_distill}
\end{figure}

\section{Adapters in Sequence-to-Sequence}
\label{sec:adapter_seq}

In this section, we divide the discussion and reviews into three sub-sections. Section \ref{sec:adapter_place} discusses the importance of adapters placement in the transformer layers as well as in the sequence to sequence framework. The first part reviews the work of \cite{houlsby2019parameter} and \cite{bapna2019simple}. Both works propose the same concept of adapters with similar architecture. The difference between them lies in the placement of the adapters within the transformer layer. The other work by \cite{winata2020adapt} discuss the importance of adapters in sequence-to-sequence framework. They found interesting results where adapters in one component have more impact than in the other component.

Section \ref{sec:app_nlu_asr} and \ref{sec:app_mt} will review the application of adapters in other fields such as NLU and ASR as well as in the machine translation. Each will discuss the type of adapters used, the purpose of the adapters, and the impact of including the adapters as a part of their training procedure.

\subsection{Placement of Adapters}
\label{sec:adapter_place}
As of this writing, there are two types of adapters placement in the transformer architecture. The first type of adapter was proposed by \cite{houlsby2019parameter}. We recall that each transformer layer consisted of two sub-layers: a self-attention layer and a feedforward layer. Each sub-layers will be followed immediately by a projection layer whose job is to transform the feature's size back to the original input size. The adapters are applied to the output of the sub-layer of the transformer. Specifically, it is applied to the output of the projection layer that transforms the vectors back to the input size and before being applied to the skip connection. The illustration of the adapter architecture can be seen in Figure \ref{img:ada_houlsby}.

\begin{figure}[h]
    {\includegraphics[width=0.95\textwidth]{img/adapter_houlsby.png}}
    \centering
    \caption{Illustration of adapter architecture. Figure reprinted from \cite{houlsby2019parameter}.}
    \label{img:ada_houlsby}
\end{figure}

The second work of adapters is published by \cite{bapna2019simple}. They take a simpler approach in contrast to \cite{houlsby2019parameter} in regards to incorporating the adapters into the transformer network. Rather than inserting two serial adapters into the sub-layers of the transformer, they instantiate a single instance of adapters and place it on the top of each layer. In addition to the simpler design of adapter layers, they add layer normalization to normalize the input of the adapters. The reasoning behind this is to make the adapters pluggable into any part of the base networks, ignoring the distribution variations of the previous layers. For illustration we refer to Figure \ref{img:ada_bapna}.

\begin{figure}[h]
    {\includegraphics[width=0.95\textwidth]{img/adapter_bapna.png}}
    \centering
    \caption{Illustration of adapter architecture. Figure reprinted from \cite{bapna2019simple}.}
    \label{img:ada_bapna}
\end{figure}

\cite{pfeiffer2021adapterfusion} mentioned that the placement of adapter parameters within a pre-trained model is non-trivial and thus requires extensive experiments. To identify the best setup for the placement of the adapter, the authors perform an exhaustive search on the hyperparameters. This includes the position and the number of adapters within a single layer, the position of residual connections, the bottleneck residual factors, as well as the non-linearity within the bottleneck adapter layer. In their experiments that involve both NLU and NLG, they find the best performing adapter is close to the simple architecture proposed by \cite{bapna2019simple} and not from \cite{houlsby2019parameter}.

We now discuss the placement of the adapters from the perspective of the encoder-decoder framework (sequence-to-sequence). \cite{winata2020adapt} conducted an experiment using similar adapter architecture and placement to \cite{bapna2019simple} in ASR task. The encoder in this work represents a model whose job is to encode audio representation. On the other hand, the decoder is responsible for encoding the text features and generating text output. In this work, the authors perform experiments comparing the effectiveness of adapters in both encoder and decoder. They initialized the decoder using pre-trained mBERT weights \cite{devlin2018bert}. Based on their findings, adapters in the decoder are not as effective as they are in the encoder component. These results indicate that adapting the weights in the audio space is more effective for the model's performance than in the text space. Features in mBERT may have already provided enough good features so that further fine-tuning may no longer be necessary.

\subsection{Adapters in NLU and Automatic Speech Recognition}
\label{sec:app_nlu_asr}
As of this writing, adapters have been used as an alternative for naive fine-tuning in various fields in NLP. \cite{houlsby2019parameter} introduces adapters to fine-tune BERT model in various NLU tasks. Primarily, they use the adapters in GLUE \cite{wang2018glue} and additional classification tasks. They conducted another experiment on a more complicated problem such as SQuAD \cite{rajpurkar2018know}. They found positive results in using adapters in GLUE, additional classification, and SQuAD tasks compared to naive fine-tuning. By adding a small number of parameters, they achieved a comparable performance in all tasks compared to fine-tuning the whole weights. We can refer to Section \ref{sec:adapter_place} for the explanation and illustration of the adapters architecture of their work.

\cite{pfeiffer2020madx} works in adapters involve bootstrapping pre-trained NLP models such as BERT, and XLM \cite{conneau2019cross} in low-resource languages. They perform two types of adaptation, language-adaptation and task-adaptation. For this purpose, they use two different adapters used in different situations. The language-adapters are trained in MLM objective to capture various features in different languages. They then put task adapters on top of the language adapters for further fine-tuning for each of the tasks in their experiments. They use an efficient adapter architecture based on \cite{pfeiffer2021adapterfusion} works which have similarities to the work in \cite{bapna2019simple}. For more details on the architecture definition, we refer the reader to Section \label{sec:adapter_place}. They perform their experiments on three different tasks: Named Entity Recognition (NER), extractive question answering (QA), and causal commonsense reasoning (CCR). These tasks are available in multi-languages that cover both high-resource and low-resource languages. We suggest the reader to their original paper for more details on the dataset they used on each task. Their main finding is that the task-specific adapters perform similar to the work in \cite{houlsby2019parameter}. However, since their main focus is on multilingual setup, they did not find satisfying results due to discrepancies in unseen languages. With the help of language adapters, they managed to bridge the missing gap and have improved performance across all tasks. The performance is especially appealing in low-resource languages.

\cite{winata2020adapt} evaluates adapters performance in multilingual ASR setup. They use a similar setup as \cite{bapna2019simple} to adapt both the encoder and decoder. They employ adapters to combat language mismatch and improve models' robustness in various languages with limited resources. Their main finding for the adapters is that it improves the performance across various languages. Furthermore, they also performed experiments to generate a cluster of languages where similar languages are put within the same group. They then share the adapters to languages belonging to the same group and find out that this helps performance in low-resource languages but has small drawbacks in the high resource languages.

\cite{lee2021adaptable} propose a multi-domain adapters for language model in ASR setup. Specifically, they employ the adapters to adapt the language model (LM) component for rescoring purposes in an ASR system. They use the similar architecture of the adapter to the work of \cite{houlsby2019parameter} where they introduce adapters within the sub-networks as well as the top of the layer. Their experiments show that by using separate adapters for each domain, they can re-use a general domain LM and switch domains by replacing adapters with the one that has already been fine-tuned on the respective domain. They also found that with the help of adapters, they manage to improve the performance on a specific problem, such as predicting proper nouns.

\subsection{Adapters in Machine Translation}
\label{sec:app_mt}
\cite{bapna2019simple} propose an alternative from \cite{houlsby2019parameter}. Instead of using two different adapters within a single transformer layer, they propose to use a single adapter on top of the layer. Another difference is adding a layer normalization after the transformer layer output. Furthermore, they also experiment with the adapters in the NLG domain instead of NLU. They use machine translation objective and treat the adapters as the adaptation module for a specific language pair. They first pre-trained the model in a large corpus such as WMT before performing domain adaptation to a smaller corpus such as IWSLT in the same language pair. In a single language pair experiment, they found out that the architecture of the proposed adapter is flexible to the size of the data by adjusting the adapter's capacity to match the requirements for the corpus size adaptation. Furthermore, they found out that they did not find a sign of overfitting; instead, they found the model to reach its peak and stay steady throughout the training process.

\cite{philip2020monolingual} applies adapters in multilingual machine translation differently than \cite{bapna2019simple}. \cite{bapna2019simple} uses adapters for every target pairs. For example, in the English France pair, they have to create two different adapters for English$\rightarrow$France and France$\rightarrow$English. \cite{philip2020monolingual} propose to use a monolingual adapter where instead of using a single adapter for a language pair, they use a single adapter for each of the languages. They use the same adapter architecture as in \cite{bapna2019simple} to perform the experiments. Their findings suggest that their approach reduced the number of adapters from $n(n-1)$ to $2n$. Furthermore, they also find that this approach has better performance in low-resource languages than the one proposed in \cite{bapna2019simple}.

\cite{guo2021adaptive} shows that adding adapters to BERT during the fine-tuning can be beneficial in the machine translation task. The purpose of the adapters in this work is to make the fine-tuning process more lightweight than the regular fine-tuning. They follow the adapter architecture and similar mechanism to include the adapters on the base model from \cite{bapna2019simple}. The difference is that \cite{bapna2019simple} do not use BERT as the base model and use a plain transformer that is pre-trained using machine translation objective. Although BERT uses transformer architecture, it possesses various information from the pre-training phase.

% The difficulties of incorporating BERT in machine translation has already explained in \ref{sec:domain_adapt}. \cite{guo2021adaptive} also propose that


% \section{Fine-tuning in Sequence-to-Sequence Framework}