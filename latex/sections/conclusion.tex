\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
In this work, we show that with the right initialization, adapters can help achieve better performance than training models from scratch while only training far fewer weights than the original model. We further show that even with random fixed weights in the main part of the model, the adapters and cross-attention can recover and achieve a similar performance to one of the baseline models where we do not need to train hundreds of millions of weights. Although the random pre-trained weights result shows an interesting result, further experiments may be required to understand the limitation of the adapters in this setup.

This work shows the importance of adapters in encoder or decoder. We find that fine-tuning adapters on the encoder side are more important than the decoder. We also see a similar behaviour when we initialize BERT only on the encoder or the decoder. Furthermore, we found an interesting behaviour when we do not fine-tune the encoder (only incorporating adapters on the decoder). We found that the model performance completely depleted to zero and can not recover.

We further continue the study to see the behaviour of adapters when we try to down-scale the pre-trained model size. In our experiments, we found that the model with smaller weights, such as \texttt{zsbert}, when fine-tuned with adapters, can outperform \texttt{zbert} in a similar setup at the later stage. Finally, we also observe that we can increase further the effectivity of adapters in \texttt{zsbert} by only incorporating adapters on the encoder side.

From this study, we can understand that even though the use of adapters in fine-tuning is straightforward, we can further optimize the framework by:
\begin{itemize}
    \item Initializing the encoder with a pre-trained model such as BERT and use the adapters on the encoder side. We can see this as a potential application when we are faced with low-resource translation text.
    \item We can see a potential in reducing the size of BERT when fine-tuned in adapters as we found that we can get almost a similar result with full-weight BERT when we only use half the size. We recommend to further experiment on this study by investigating more different method to reduce the BERT size.
\end{itemize}
