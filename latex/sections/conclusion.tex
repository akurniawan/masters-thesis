\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
This thesis has explored various ways to utilize BERT with adapters in machine translation. We start by understanding the impact of pre-training data in different domains and the contribution of different volumes in the pre-training data. We continue the study by leveraging different techniques to understand the impact of the pre-trained representations by shuffling the original BERT weights and using randomly set weights. We then conduct the experiments to understand the importance of adapters in either the encoder or the decoder by showing the performance of adapters when they are removed from either of the components. Finally, we perform reduction experiments where we reduce the size of BERT by manually removing the weights either by zeroing out some of the values in the matrices or completely deleting them.

Experiments in Chapter \ref{chap:adaptmt} show that with the proper initialization, adapters can help achieve better performance than training the models from scratch while training far fewer weights than the original model. We further show that even with random fixed weights in the main part of the model, the adapters and cross-attention can recover and achieve performance similar to one of the baseline models.

In the subsequent experiments in Chapter \ref{chap:adaptefct}, we find that fine-tuning adapters on the encoder side is more important than in the decoder. We also see a similar behaviour when we use the original BERT weights only on the encoder or the decoder and fixed random weights on the other part. Interestingly, when the adapters were injected only to the decoder, with the encoder pre-trained or random, the performance dropped to zero. In other words, a fitting encoder is critical.

We further studied the behaviour of adapters when we tried to down-scale the pre-trained model size. In our experiments, we found that the model with just half of the weights, such as our \texttt{zsbert}, can closely match the performance of the baseline model, the model that uses BERT in both of the encoder and the decoder and only fine-tuning the cross-attention and output layers. Finally, we also observe that we can increase further the effectiveness of adapters in \texttt{zsbert} by only incorporating adapters on the encoder side.

We see two practical applications from our findings:
\begin{itemize}
    \item Initializing just the encoder with the pre-trained weights such as BERT (with a fixed random decoder) and fine-tuning with adapters could be helpful when targeting low-resource languages. The random decoder is created trivially and no large target side monolingual corpus is needed.
    \item Reducing the pre-trained BERT to half its size and fine-tuning with adapt\-ers provide useful GPU memory savings while keeping a similar performance as the baseline model.
\end{itemize}

In summary, our experiments show the potential of adapters in machine translation setup. We understand from the experiments that fine-tuning adapters with randomly set weights in the base pre-trained network can achieve similar results as training the entire transformer model with BERT configuration. Furthermore, the down-scaling experiments also show that with a random weight reduction technique, we can reduce the size of BERT and achieve similar performance as the BERT model that was fine-tuned by only modifying the cross-attention layer.